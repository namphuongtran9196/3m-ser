{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "import torch\n",
    "\n",
    "from bertviz import head_view, model_view\n",
    "from configs.base import Config\n",
    "from transformers import BertTokenizer\n",
    "from models.networks import MMSERALayerNorm\n",
    "from data.dataloader import build_train_test_dataset\n",
    "\n",
    "opt = Config()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "device = torch.device(\"cpu\")\n",
    "network =torch.load(\"/home/kuhaku/Code/EmotionClassification/code/3m-ser-private/scripts/checkpoints/MMSERALayerNorm/20230607-044147/weights/best_acc/checkpoint_0.pt\",map_location=torch.device('cpu'),)\n",
    "# network = MMSERALayerNorm(num_classes=opt.num_classes, num_attention_head=opt.num_attention_head, dropout=opt.dropout)\n",
    "train_ds, test_ds = build_train_test_dataset(\"/home/kuhaku/Code/EmotionClassification/code/3m-ser-private/scripts/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anger      1.0 There is no point in coming down here.\n",
      "Neutral    0.0 My name is Sara.\n",
      "Neutral    1.0 Everyone wants to be a working actor, but they all work at something else.\n",
      "Neutral    1.0 Well you know sometimes-- just with online transactions, things happens. so um-\n",
      "Sadness    1.0 there's people that thought it was the right thing to do so now we have to- we have to do what we have to do for our country\n",
      "Sadness    1.0 He just put it in my pocket that's the kind of guys I had.\n",
      "Neutral    1.0 Alright?  Great.  Have a nice evening.\n",
      "Neutral    1.0 I understand that. Um- So I'm going to put down here that you looking for your baggage, but we actually can't, uh this bag is likely not going to show up so what we're going to do-\n",
      "Happiness  1.0 I'll never forgive you for that.\n",
      "Anger      1.0 Amanda listen. Listen.\n",
      "Neutral    0.0 It's become a habit with you.\n",
      "Neutral    1.0 Is this a Masters or a P.h.D.?  Or can you...? Nice\n",
      "Neutral    1.0 That obviously that's not true though.  I mean, you've done this before.  You've done all the work like this before, you know, and and you've gotten people saying oh, you're so good and like you know--\n",
      "Neutral    0.0 Look at this, goose bumps.\n",
      "Sadness    1.0 And I got this idea watching them go down, everything was being destroyed, see?  But, to me, it seemed there was one new thing that was made,\n",
      "Neutral    0.0 Absolutely.\n",
      "Neutral    1.0 Um, well, sometimes there is just difficulties with loading on to the plane. Uh- There can be a theft or uh- some sort of you know mishap with a cart or something.\n",
      "Happiness  1.0 How you doing? You know what I forgot? I forgot to bring my flashlight. How - how could I forget to bring my flashlight? How could I be so stupid?\n",
      "Sadness    1.0 I would rather not remember some things.  I'd rather not hope for something.\n",
      "Anger      1.0 Okay.  You understand that's all I want. Just don't give me a problem with this.  Okay, please because I'm very sensitive right now\n",
      "Happiness  1.0 So what do you think?\n",
      "Anger      0.0 We didn't see them the first time either.\n",
      "Sadness    1.0 Just about all.\n",
      "Neutral    1.0 Wait, that's my point, too.\n",
      "Happiness  1.0 Oh, thank you. [LAUGHTER]\n",
      "Neutral    0.0 I brought- I brought Chuck with me. He went to Redlands, yeah, for the day.  We ran through the sprinklers.\n",
      "Happiness  1.0 What would I do with a fortune?\n",
      "Anger      0.0 Oh, yes I am. Yes I am and you can't stop me.\n",
      "Neutral    1.0 Chances are it's probably just you know -- taking time being um- unloaded from the airplane.\n",
      "Sadness    1.0 I'm just so scared to start over I mean.  I put so much time and energy into being here.\n",
      "Anger      1.0 Yeah but you weren't unemployed for three years trying to find any job.  I mean at this point I'm going to go drive a cab or, you know, work at the gas station or something.  I don't even- I don't know what to do anymore.\n",
      "Sadness    0.0 But it- it is for a year.\n",
      "Happiness  1.0 Yeah.  It will work out.  Maybe I could work at Red Lobster.\n",
      "Neutral    1.0 Yeah, that's my point too.\n",
      "Happiness  1.0 Well Vegas was awesome.\n",
      "Sadness    1.0 She was with- yeah  She was with Gary, and he said that she was just sitting there and went to sleep, closed her eyes, and that was it.\n",
      "Happiness  1.0 This is, this is great, isn't it? I wouldn't miss this for anything. Now listen to this, I know you're not interested but just listen.\n",
      "Happiness  1.0 I know.  Right down the street.\n",
      "Sadness    1.0 You still feel that way?\n",
      "Anger      1.0 Well then you'll wake up the people downstairs.\n",
      "Neutral    1.0 'Cause if you've been looking, you've been applying for jobs for three years. And-\n",
      "Happiness  1.0 find out all this stuff.  And- and I don't know.  I think I'm going to try to live in a dorm.\n",
      "Anger      0.0 That was the first time you ever hit me.\n",
      "Neutral    0.0 Uh- when's when's-\n",
      "Neutral    1.0 Well it didn't harm him, in fact, it pleasured him a lot and it didn't hurt me.\n",
      "Happiness  1.0 Well yeah, that's the general idea. [LAUGHTER]\n",
      "Happiness  1.0 No.  But I'm thinking I might, you know, like audition for a play or something.  I don't really want to do the sorority thing.  I'm not really feeling that vibe with the\n",
      "Anger      1.0 Well, what do you want me to do?  You're old enough known your own business.\n",
      "Neutral    1.0 Okay.  How about this we'll send you a brand new camera phone with palm pilot-- and-\n",
      "Happiness  1.0 I didn't even think to bring a six-pack.  A six-pack would be just the thing right about now. I can't believe I forgot.  And look at kid, I am surprise I make it out of the house about flies it.\n",
      "Happiness  1.0 Well, I mean I just told her--Well I just said, you know, hey, these are my suggestions.\n",
      "Happiness  1.0 Oh, what, how- where how did you do it?\n",
      "Anger      1.0 Twice is every time we've tried, that's ever\n",
      "Sadness    1.0 something happens and they die and it's like-\n",
      "Happiness  1.0 And very much sillier.\n",
      "Sadness    1.0 Not sorry, no.  I can't stay here.\n",
      "Happiness  0.0 Oh yes, they - they you know they love her and so I mean...\n",
      "Happiness  1.0 Yeah.  Hopefully it won't be like high school.\n",
      "Happiness  1.0 Of course, of course she will ... of course she will.  Yeah, don't go and disappear like everyone else after they get married and I never see them again.  That'll make me mad.  You don't want to make me mad.  You know how I get when I get mad.\n",
      "Anger      1.0 Probably a hundred people told her she's foolish but she's waited.\n",
      "Happiness  1.0 Oh, I approve. I-[LAUGHTER]-I more than approve.\n",
      "Happiness  1.0 With perfect poise...\n",
      "Sadness    1.0 Ah.\n",
      "Happiness  0.0 Come on, what?\n",
      "Happiness  1.0 The flashlight.  The silver one.  There's only one, isn't there?  Stupid.\n",
      "Neutral    1.0 How long did you know each other?\n",
      "Sadness    1.0 For instance, one time, it had been raining several days and this kid came to me and gave me his last pair of dry socks.  I put them in pocket.  It's only a little thing, but that's the kind of guys I had.\n",
      "Anger      1.0 She's not his girl, Joe.  She knows she's not.\n",
      "Happiness  1.0 The manager came in and found is rolling about on the floor, biting and scratching like panthers.  Oh, dear.\n",
      "Sadness    1.0 This is just this.  You know?  I mean it's a lot and everything but it isn't somewhere else.  Do you know what I'm trying to say?\n",
      "Neutral    1.0 Obviously, we're not trying to lose your luggage.  Do you really believe that we're trying to lose your luggage ma'am?\n",
      "Happiness  1.0 Oh, Charles; that was his name, Charles.  Oh, he did wriggle so beautifully.\n",
      "Sadness    1.0 You're the one who probably wishes he was with somebody else.  Somebody who doesn't take things so seriously, who could enjoy herself.\n",
      "Happiness  1.0 Perfect!  Oh, my God.  Are you staying, are you going to live on campus?\n",
      "Anger      0.0 like I said before right now.\n",
      "Happiness  0.0 Right, being like oh, where are you from?  The U.S.  Where in the U.S.?  Chicago.  Where in Chicago?\n",
      "Sadness    1.0 No, I know. But still, I know I don't make you happy.\n",
      "Anger      1.0 You are far too temperamental.  Try to control yourself.\n",
      "Happiness  1.0 Charles.  That was his name.  He did wriggle so beautifully.\n",
      "Anger      1.0 What about the business? What is this?\n",
      "Neutral    0.0 Yeah, it's stressful.  I recognize that.\n",
      "Happiness  1.0 She actually asked if it was real?  I would've probably asked the same thing considering you were in Chuck E. Cheese.\n",
      "Happiness  1.0 Um- And she wants you to be in it, too.  She loves you so much. She thinks you're the sweetest.  Of course, you are.  You're You're my best friend so-\n",
      "Anger      1.0 Would you uh...turn that off please? You're going to wake the people upstairs.\n",
      "Neutral    0.0 Why?\n",
      "Anger      0.0 Last year you'll also remember our shoes got wet, I got sand in my panties, you got a sore throat, we got into a big fight over nothing, and we didn't see the Grunions.\n",
      "Anger      1.0 Please just put the form in, take the picture, let me get my license and get the hell out of here.  Please.\n",
      "Neutral    1.0 I mean are you going in for interviews?\n",
      "Sadness    1.0 Yeah.  We were.  Pretty close.\n",
      "Happiness  1.0 I kissed you.\n",
      "Happiness  1.0 Oh yeah. He'd drop you right off at practice?\n",
      "Neutral    0.0 That's good.  Don't you have to like teach a class, too, when you do that?\n",
      "Happiness  1.0 Oh, my Gosh. that's so cute\n",
      "Anger      0.0 If your life is going to be in danger and this family is going to be in danger we don't need the job. We'll find the money.\n",
      "Sadness    0.0 No, you don't.  I'm a pretty tough guy.\n",
      "Happiness  0.0 Why the sad face?\n",
      "Happiness  1.0 No but that's an awesome school. That's so cool.\n",
      "Happiness  0.0 Yeah, yeah. She was actually -- she was going to tell you but I told her that I wanted to tell you and she had to work today anyway, so I decided to-\n",
      "Happiness  1.0 Ah, it send shivers up my spine.\n",
      "Sadness    1.0 Hey sweetie.\n",
      "Happiness  1.0 No.  Wrong number. [LAUGHTER]\n",
      "Neutral    0.0 Do they always run during a full moon?  I don't remember.  Was it full last year?\n",
      "Sadness    1.0 We're going to be okay.\n",
      "Neutral    1.0 Great.  How many seats?\n",
      "Anger      1.0 You are far too temperamental, try to control yourself.\n",
      "Neutral    1.0 I don't know, sir.\n",
      "Neutral    0.0 Going out somewhere, dear?\n",
      "Neutral    1.0 How do you know he's even thinking about it?\n",
      "Neutral    0.0 So maybe we are in the wrong spot, but we are with the right person.\n",
      "Sadness    1.0 Well you have to have faith because I don't have a choice and I have to go.\n",
      "Sadness    1.0 He was my only friend growing up.\n",
      "Anger      1.0 Who--you always use your driver's license.  I mean, you're not coming to the D.M.V. if you're not over sixteen and that's when you get your driver's license so you're never gonna need it anyway.\n",
      "Neutral    1.0 You're going to have to just be patient with us, sir, as we\n",
      "Sadness    1.0 I don't know.\n",
      "Happiness  1.0 Well, so what do you think?\n",
      "Happiness  1.0 know you applied to U.S.C..\n",
      "Sadness    1.0 You guys were pretty close, weren't you?\n",
      "Happiness  0.0 You know the area better than I do.\n",
      "Neutral    0.0 Is there a funeral planned? or-\n",
      "Anger      1.0 Amanda-- Listen- Listen-\n",
      "Neutral    0.0 Do you still feel like that?\n",
      "Happiness  0.0 Eh, we can work that part out.\n",
      "Sadness    1.0 Yeah.  I know.\n",
      "Sadness    1.0 Sweetheart, I've got to tell you something.  I just got a call.  I'm going to Iraq.\n",
      "Happiness  1.0 Yeah, but that moment on the beach, is the highlight of their little lives.  All that flopping is laying eggs or what do you--spawning or mating.  I mean,\n",
      "Neutral    1.0 And you said let's get some champagne and I said time's a wasting or something like that.\n",
      "Anger      1.0 It's PR.  I mean, somebody sold you a good bill about this wonderful event that going to happen on the beach.\n",
      "Sadness    1.0 Do promise me you're going to take a lot of pictures.\n",
      "Sadness    0.0 I'll tell them I'm sick and that you can't leave something.\n",
      "Neutral    1.0 That's the woman's job to lure the man.  Watch me a moment won't you?\n",
      "Neutral    1.0 Like- What's your ideal job?  What do you want to do?\n",
      "Happiness  1.0 Yes, meaning just that.\n",
      "Sadness    0.0 Well, if she does, then that's the end of it.  But from her letters, I think she's forgotten him.  I'll find out.  And then we'll thrash it out with Dad, right?  Mom, don't avoid me.\n",
      "Neutral    1.0 I think that's kind of stupid, that you put a hundred thousands of dollars in the bag.\n",
      "Sadness    1.0 Takes a little time to toss something like that off you know because they weren't just men-\n",
      "Neutral    1.0 Yeah, well the chances of it being lost forever, you know, are slight because -- -- you were in the --\n",
      "Happiness  1.0 And look what I got here.\n",
      "Sadness    1.0 My friend, Shotty, passed away.\n",
      "Anger      1.0 Well that's a good answer but it don't answer anything.  You haven't seen her since you went to war.  It's been five years.\n",
      "Sadness    1.0 You gotta take care of our baby for me, alright?\n",
      "Anger      1.0 If you're- if you're- if you're looking for a lawsuit, baby, you've found it.\n",
      "Sadness    0.0 I don't want you in danger.  And I don't want her in danger.\n",
      "Sadness    1.0 I know that you meant this to be special and nice and the sand and the moon and all, but I just couldn't help thinking about being somewhere else.\n",
      "Happiness  1.0 Yeah.  Can you believe that? Yeah, she's like hey, I'm like sure I'll do it, I don't care. Yeah. I'm like fifty thousand dollars, I'd totally do it for fifty thousand dollars. But the only thing is that's kind of like illegal. So like I could go to jail probably for a long if I get caught, but...\n",
      "Neutral    1.0 Yeah, hi.  Um, look, I -- all I'm trying to do right now is renew my license plate.  That's all.  I just need to get one of those little stickers.\n",
      "Neutral    1.0 Yeah, I understand, but-\n",
      "Neutral    1.0 Only a trivial little broach.\n",
      "Happiness  1.0 They have - U.S.C. has everything because U.S.C. is the best school ever.\n",
      "Happiness  1.0 Are they so excited?\n",
      "Sadness    1.0 Yes.  Don't you know that? I mean, You're probably the one you wishes you were with somebody else, somebody who didn't take everything so hard and who knew how to enjoy herself.\n",
      "Neutral    1.0 Wow.  I don't -- I mean, do you- do you have an attitude problem when you're on the job?\n",
      "Anger      1.0 A new carpet, a poodle, a suicide pact, ice cream, a back rub, what?  What?  What the hell do you want?\n",
      "Happiness  1.0 That is amazing, congratulations. Good for you.\n",
      "Neutral    1.0 I don't get it.  The first time we came here, you said it was the best night of your life.\n",
      "Happiness  0.0 like a loser, I cried.\n",
      "Happiness  0.0 Yeah and they said get whatever you want and...\n",
      "Happiness  1.0 [LAUGHTER] Why did she pick you? Where were you?\n",
      "Neutral    1.0 Do you want to go home?  Are you cold?\n",
      "Neutral    1.0 Well.\n",
      "Happiness  1.0 Ooh, certainly you must. We're figures of fun, all right.\n",
      "Neutral    1.0 Fine.\n",
      "Happiness  1.0 I know you did. You threw it out of the window into the Grand Canal; I don't think I'll ever forgive you for that.\n",
      "Neutral    0.0 'cause, you know, I mean, it's like If I ever want to go south of campus\n",
      "Neutral    1.0 You have to be willing - You have to be willing to start you know at the bottom and work your way up. and\n",
      "Happiness  1.0 Look at this, all of the hairs are standing up on my arm.\n",
      "Sadness    1.0 There's so many things mixed up.\n",
      "Happiness  1.0 Honey, this is a natural phenomenon. I mean,  It happens like once a year.  It's a great opportunity.\n",
      "Happiness  1.0 Oh, yes meaning just that.\n",
      "Neutral    1.0 which you- which you'll have to go into a local Verizon dealer--  or Sprint.\n",
      "Anger      1.0 You're not going like this.\n",
      "Sadness    1.0 I know that, Augie.  Really, I do.\n",
      "Neutral    0.0 But look at the--he--look at the life he did get to lead.  I mean, for his age, he'd done so much and--\n",
      "Anger      1.0 Oh dear, you really are becoming quite insufferable.  I would imagine it's because you're drunk.\n",
      "Anger      1.0 I ignore what I got to ignore.  I mean, the girl is Larry's girl.\n",
      "Anger      1.0 Why?\n",
      "Neutral    1.0 That's helpful.\n",
      "Sadness    1.0 No, I know.  But still.  I know I don't make you happy.\n",
      "Happiness  1.0 Now that you mention it. No, I don't.\n",
      "Neutral    0.0 Just seems so silly to go on and on and on with such a thing.\n",
      "Anger      1.0 Turn it off.  It's driving me mad.\n",
      "Happiness  1.0 Oh, and you sit down and you have the sponge bread-- --and eat the--drink the beer. Ugh, I love that stuff. I'm so excited.\n",
      "Happiness  1.0 [LAUGHTER]. I'm so excited, Joe. Oh, man, I-- And I got in--I got into the school of theatre, so it's like-\n",
      "Sadness    1.0 Yes.  [BREATHING] God.\n",
      "Sadness    1.0 He was only twenty four and just so many things that we were going to do, you know, and now he won't get to do them and he won't get to be here when I do.  You know, like I'll get married some day and he won't be there, like, he won't get to see that.\n",
      "Anger      1.0 Stop.  I hate you.  Do you hear me? You're conceited and overbearing and utterly impossible.\n",
      "Anger      1.0 You're not going like this.\n",
      "Happiness  1.0 Yeah.  Totally- Well where are you going to live?\n",
      "Anger      1.0 Look at you, you're shaking.\n",
      "Happiness  1.0 Yeah.  We'll have so much fun.\n",
      "Anger      1.0 But don't think that way, you hear me?\n",
      "Sadness    1.0 What's that supposed to mean?\n",
      "Happiness  1.0 What shall we do if they suddenly walk in on us?\n",
      "Anger      1.0 What?  It's my phone.  You can't even call me on it. I don't have another phone.\n",
      "Neutral    1.0 um, you're scheduling a flight as well--And you would like to, um, we have a highly discounted hotel rate that goes along with that so that everything is all in one deal.  You're taken care of by the time that you get there.\n",
      "Neutral    1.0 I'm very sorry but let's check on this immediately.  This is definitely an emergency.  It seems--\n",
      "Anger      1.0 I'm not frustrated.\n",
      "Happiness  1.0 Oh, well look what we've got here.\n",
      "Happiness  1.0 [LAUGHTER] Thanks, yes, thanks, yes That'll be cool.\n",
      "Neutral    1.0 No.  You can- or you can just bring it in person.  But they need, generally, something to see, you know, what kind of what that you've done, what kind of--\n",
      "Neutral    1.0 I'm sorry it has to do with an electrical power outage and stuff--\n",
      "Happiness  1.0 [garbage] I know, I know, I know, we'll figure it out.  Right?  [garbage] it is going to be so much fun.\n",
      "Sadness    1.0 A little more selfish and they'd still be here today.  And I got this idea watching them go down.  See everything was being destroyed but there was one new thing that was being created.\n",
      "Happiness  1.0 And she's like- she feels it you know and she doesn't know what it is.  And she you know helps me get out and I won't let go of her hand.  And she's like, what are you doing?  I and said, You saved my life, now will you marry me?\n",
      "Neutral    0.0 How long did that row last?\n",
      "Happiness  1.0 The one where you bought that little painted wooden stake and you put it on my bed.\n",
      "Neutral    0.0 I am not the only one, look at all these people.\n",
      "Anger      1.0 Well, I am thinking like that.\n",
      "Anger      1.0 This is the worst customer service I've ever gotten before.\n",
      "Anger      1.0 No restraint at all, very enjoyable, never had much anyhow.\n",
      "Sadness    0.0 Sure, this is standing.  This is waiting.  This is fighting.\n",
      "Happiness  1.0 Shh.  If we're very quiet the fish might come.\n",
      "Sadness    1.0 It's weird.  Whenever I go back to town, I think of her.  I mean, it's like ... I mean we grew up together and she died when we were twenty one.  And yeah.  She had cancer, she had two brain tumors.\n",
      "Neutral    1.0 Well do you think you're going to live on campus or off campus your first year?\n",
      "Anger      1.0 You're not going to call the other operator. You're not going to call the the other operator.\n",
      "Neutral    0.0 I know. I understand it's personal-- It's personal to you. I get that.\n",
      "Anger      1.0 I'm not in the least drunk.\n",
      "Neutral    1.0 Uh huh Uh huh We want you to be happy.  You're our customer.  You're the most important thing. Okay?\n",
      "Happiness  1.0 Oh Charles, that was his name.  He did wiggle so beautifully.\n",
      "Neutral    1.0 We missed them twice.\n",
      "Happiness  1.0 okay.  Well I'm not going to be undecided forever. [LAUGHTER]\n",
      "Sadness    0.0 How's, um -- how's his parents doing?\n",
      "Neutral    1.0 Is there anything I can get you?  Do you want me to take care of anything for you or...?\n",
      "Anger      1.0 Okay.  Get on the phone and figure out where my luggage is and tell me when it's going to get there alright?\n",
      "Happiness  0.0 I have some better news than that.\n",
      "Happiness  1.0 I'm going to make a fortune for you.\n",
      "Anger      1.0 Do you understand who my husband is?\n",
      "Neutral    1.0 I am seeing the two hundred dollar charge.\n",
      "Neutral    1.0 No I'm just making myself fascinating for you.\n",
      "Neutral    1.0 As a matter of fact, the real cause of that row was Peter Burden.\n",
      "Neutral    1.0 I mean, it's not big or anything like that.  I mean, it's not worth a lot of money or anything.\n",
      "Happiness  1.0 We're going to live now.  I'm going to make you so happy.\n",
      "Happiness  1.0 Yeah.\n",
      "Anger      0.0 I don't see why you want any you already had two glasses.\n",
      "Happiness  1.0 I'm-I'm looking forward to that. [BREATHING] man, So yeah, I think I'm going to come out a little early so I mean I'll give you a call and we'll definitely-\n",
      "Sadness    1.0 They didn't die.  They killed themselves for each other.  I mean that exactly.  A little more selfish and they'd have been here today.\n",
      "Neutral    1.0 The worst one was in Cannes when your curling irons burnt holes in my dressing gown.\n",
      "Sadness    1.0 He actually wanted to do something.\n",
      "Sadness    1.0 So you're leaving tomorrow.\n",
      "Happiness  0.0 I love you a great deal.\n",
      "Neutral    0.0 Well, I mean, it makes sense, doesn't it, hmm?\n",
      "Neutral    1.0 Well that's only your business, Chris.\n",
      "Sadness    0.0 right into my ear so that I could hear them as much as- so that I could feel them as much as hear them.\n",
      "Happiness  1.0 Yeah.  Yeah, of course.\n",
      "Neutral    1.0 Huh.  Are you cold?  Do you want my jacket?  No?  you know, we should have brought the blanket?  Our blanket.\n",
      "Sadness    1.0 Yeah, I'm just-- yeah\n",
      "Neutral    0.0 Okay.  Well let's...well I'm going to go ahead and check on that for you.  You're coming from...where are you coming from?\n",
      "Happiness  1.0 Total turn on.\n",
      "Anger      1.0 It's not like I can go to Kmart and buy another.\n",
      "Anger      1.0 Because you know what you get, Carla?  You know what you get?  This.\n",
      "Anger      1.0 Well, what about me?\n",
      "Sadness    1.0 I'd rather not remember some things.  I would rather not hope for some things.\n",
      "Sadness    1.0 you know over seas I was in command of the company?\n",
      "Neutral    1.0 Well, um- we have a little problem with the system going on.  One of our satellites is down, and I-\n",
      "Neutral    1.0 Well, what of it?\n",
      "Happiness  1.0 Yes, well, Thanks, thanks. Thank you. yeah\n",
      "Sadness    1.0 You can't leave your daughter alone like this.\n",
      "Anger      0.0 We didn't see them the year before either.\n",
      "Neutral    0.0 That's- I mean. There are more factors then just whether you're good or not, you know what I mean like\n",
      "Happiness  1.0 Oh god! You are damn cheap, cheap, cheap. Anyways, moving on. It's not like you're poor.  I mean your dad makes... okey\n",
      "Neutral    1.0 I'm going to ask her to marry me.\n",
      "Sadness    0.0 And he just got that baseball scholarship.\n",
      "Neutral    1.0 Because it is.\n",
      "Neutral    1.0 Okay.  Great.  I'm really sorry that you had to do that.  What's your last name?  Let's start there.\n",
      "Neutral    1.0 Sir, can- can you tell me which branch you went to.  We can start there.\n",
      "Happiness  1.0 You don't look like you're going to faint.\n",
      "Neutral    0.0 Well, you know what?  Life gets humili... humiliating at times.\n",
      "Anger      1.0 The girl is not Larry's.  If the girl is Larry's girl she's not his she know she's not.\n",
      "Neutral    1.0 No, it doesn't pay the bills, but it would pay something.  And it would help you get somewhere else.\n",
      "Sadness    1.0 You remember overseas I was in command of a company?\n",
      "Happiness  0.0 uh- the- Being flippant brings out the acid in their damned beauty and light.\n",
      "Happiness  1.0 Yeah, so [LAUGHTER] He's calling me now\n",
      "Happiness  1.0 Fair for you, Wow, oh my Gosh.\n",
      "Sadness    1.0 I don't want to be a single mom.\n",
      "Happiness  1.0 Utterly, utterly ridiculous.\n",
      "Neutral    1.0 Well, I mean, right now I'm just doing a little bit of everything.  And I don't- I don't think that I'm in like a profession, like career oriented place.\n",
      "Neutral    1.0 Well I was kind of planning of sneaking up on you over the course of a week or so.  They just take it for granted that we're all set.\n",
      "Sadness    1.0 They want to deploy me next week.\n",
      "Anger      1.0 Yes I did, quite a lot.\n",
      "Anger      1.0 She's not his girl, Joe.  She knows she's not.\n",
      "Neutral    1.0 I want to ask her to marry me.\n",
      "Happiness  0.0 Hello.  Thank you for calling Dell Taco Corporate.  Can I help you?\n",
      "Neutral    1.0 I mean, it's just, it's really--I don't know, yeah, it's about their opinions and it's about, I don't know, just getting yourself out there and--\n",
      "Happiness  1.0 What time is it?  God, this is great, isn't it?  I can't believe it.\n",
      "Sadness    0.0 What the...what am I doing?\n",
      "Happiness  1.0 Every day since.\n",
      "Sadness    1.0 No, I know.  But still.  I know I don't make you happy.\n",
      "Happiness  1.0 Oh my Gosh, that's so cool.\n",
      "Neutral    1.0 And then we'll thrash it out with father.  Okay Mom?  Don't avoid me.\n",
      "Anger      1.0 A-a back rub, some ice cream, a suicide pact. What the hell do you want, Carla?\n",
      "Neutral    1.0 Yeah, I understand. maybe What jobs are we talking about?\n",
      "Anger      0.0 Tell me, you'd leave the business?\n",
      "Happiness  0.0 And yeah, well, I mean, basically I just applied for any and every possible scholarship and grant that could come by, so I can go.\n",
      "Anger      1.0 This is the end.  Do you understand, the end?  Finally and forever.\n",
      "Sadness    1.0 Otherwise, what you have here is loot; and there's blood on it.\n",
      "Neutral    1.0 Hi, How can I help you?\n",
      "Sadness    1.0 Yeah, they're going to do some sort of memorial service or something.\n",
      "Happiness  1.0 She wants like eight- She wants like eight kids and I'm like, no, no, no, no, no.  I am like what Let's do maybe three and we'll go with that.\n",
      "Happiness  1.0 I know you did.  You threw it out of the window into the Grand Canal.  I don't think I'll ever forgive you for that.\n",
      "Anger      1.0 Yes, I did quite a lot.\n",
      "Sadness    1.0 This is just this.  I mean, it's a lot and everything.  But it's not, you know, something else.  Am I making any sense?\n",
      "Sadness    1.0 And Uh- Yeah. Uh- They're going to need um- more nurses on call for uh- to go overseas to help out with the war.\n",
      "Sadness    1.0 I'm sorry. I-\n",
      "Sadness    1.0 I mean, that's-that's only a little thing, but that's the kind of guys I had. They didn't just die. They killed themselves for each other. And I mean that exactly. A little more selfish and they'd still be here today.\n",
      "Happiness  0.0 I love you.\n",
      "Sadness    0.0 Yeah.  I can see that.\n",
      "Neutral    0.0 Elliott, worms don't pop.\n",
      "Happiness  1.0 You'll be like are you a fan or a student you know? Student. [LAUGHTER] Oh man, that's awesome.\n",
      "Happiness  0.0 oh-\n",
      "Sadness    1.0 I don't know. I just don't.\n",
      "Happiness  1.0 Yeah, I would love to. Thanks. [LAUGHTER]\n",
      "Happiness  1.0 Finally.  I'm so excited.  We're gonna -- well, I'll be here.  We're still gonna be able to see each other.\n",
      "Neutral    1.0 if you're ever online or anything like that I mean just make sure it's a secure site and\n",
      "Sadness    1.0 To show that, to bring that onto the Earth again, like some kind of monument and everyone can feel it standing behind him and it would make a difference to him.\n",
      "Sadness    1.0 I don't know. When it cracked he ran into the house and cried in the kitchen.\n",
      "Happiness  0.0 Oh, you don't like that. You're west-sider?\n",
      "Neutral    1.0 Yes.  The real cause of that rile was Peter Burden.\n",
      "Anger      1.0 But we have the tags on. I mean like it wouldn't you know, if it got stuck somewhere it's like well this is nobodies bag I mean maybe you know look on the tag and\n",
      "Neutral    0.0 Yeah, I know I was in the country.  I mean, it's not that complicated.\n",
      "Anger      1.0 Isn't it your business, too, if dad -- if I tell dad and he throws a fit about it?  I mean, you have such a talent for ignoring things.\n",
      "Anger      1.0 The business?  The business?  It doesn't inspire me?\n",
      "Happiness  1.0 Look at the night we got; couldn't be better.\n",
      "Anger      1.0 Well, welcome to the human race.  Do you think this is what I had in mind when I proposed?  That four years down the road, we'd be at the beach yelling at each other over fish?\n",
      "Neutral    1.0 I work for IBM.\n",
      "Happiness  1.0 The exact right guy, I fully approve.  He's a wonderful guy.  We have a great time.  He can drink.  That's awesome.\n",
      "Neutral    0.0 I am thinkin' that way.\n",
      "Neutral    1.0 Well you're a considerate fellow, there's nothing wrong with that.\n",
      "Neutral    0.0 No, don't think like that.\n",
      "Anger      1.0 Well, that's a nice point of view I must say.\n",
      "Anger      1.0 Not to mention I have my work in there.\n",
      "Anger      1.0 I don't want to argue with him but it's time that he realizes that Larry's not coming back anymore.\n",
      "Sadness    1.0 I mean I haven't even talked to him on the phone for the past three months.\n",
      "Neutral    0.0 I mean, technically, that's policy.  But, like, I mean, I don't really care about -- nobody really cares about policy here.\n",
      "Neutral    0.0 they have that as a major?\n",
      "Neutral    1.0 Yes. On this I would.\n",
      "Happiness  1.0 dim down the lights and uh, I got on one of the tables pulled her up.\n",
      "Anger      1.0 Turn it off.  It's driving me mad.\n",
      "Happiness  1.0 How long have you guys been going out?\n",
      "Sadness    1.0 Yeah.\n",
      "Happiness  1.0 What time is it?  They're supposed to run around midnight.  This is great isn't it?  Look at this night we've got here.  It couldn't be better.\n",
      "Happiness  1.0 I know they have a really, really great program for what I want to do.\n",
      "Neutral    0.0 Yeah.\n",
      "Neutral    1.0 Uh--\n",
      "Sadness    1.0 I understand.  I'm worried about you.  I don't know what's going to happen to you.\n",
      "Happiness  1.0 Annie I am going to make you a fortune.\n",
      "Sadness    1.0 it's not funny. I just don't know what I'm going to do.\n",
      "Sadness    1.0 Otherwise all you have is really loot and there's blood on it.  And I didn't want to take any of that.  And I guess that included you.\n",
      "Happiness  1.0 I don't know.  Seems like a pretty good spot to me I mean look at the view of the moon you got from here.\n",
      "Happiness  1.0 Seemed like a pretty good spot to me. I mean, look at the view of the moon we've got from here.\n",
      "Anger      1.0 What of it?\n",
      "Neutral    1.0 We're not twenty one yet, but whatever.\n",
      "Happiness  1.0 I- They have like--they just have so many like prestigious like this school is top ten and like the departments within it, you know what I mean.\n",
      "Sadness    1.0 It's almost time for me to go.\n",
      "Happiness  1.0 Well, so, what do you think?\n",
      "Neutral    1.0 Is he an actor too, or-?\n",
      "Happiness  1.0 Yeah.  And she said yes.  And she cried, of course.\n",
      "Happiness  1.0 You cold, you want my jacket? We should've - we should've brought our blanket, our blanket, how could we forget to bring the blanket?\n",
      "Anger      1.0 What web- I didn't go to a website - why would I need to - it's the D.M.V.. You come to the D.M.V., you wait in line for three hours, and then you go home.\n",
      "Sadness    0.0 Just like it.\n",
      "Sadness    1.0 When I was seven, my- my grandmother died and I really- I don't know, I mean it was really hard to let go but the important thing is- is that you do let go and your realize that, you know, they- you know, they- this person had fun while they were- while they were here and, uh, you know, you work- you work through it.\n",
      "Neutral    1.0 Yeah.  It's alright.  But--\n",
      "Happiness  1.0 They do it's--believe me it's huge.\n",
      "Happiness  1.0 Well what'd you major in?\n",
      "Sadness    1.0 Just seems so unfair, is the way it is.\n",
      "Anger      0.0 Why did you invite her here?\n",
      "Sadness    1.0 It's not the same though.\n",
      "Anger      1.0 Are you crazy?\n",
      "Happiness  1.0 Cool, perfect.  Another Chicagoite, got to love it.\n",
      "Happiness  1.0 I don't really have any imagination, that's all I know how to say.\n",
      "Happiness  1.0 Look, is that...no is that just seaweed?\n",
      "Anger      1.0 I'm testing your patience?\n",
      "Happiness  1.0 Actually, now that you mention it, no.  I don't.\n",
      "Neutral    1.0 Actually that's perfectly true.\n",
      "Sadness    1.0 It was a kind of responsibility, man for man.  You understand me?\n",
      "Sadness    1.0 He sick for awhile?\n",
      "Sadness    1.0 I don't know how to start.\n",
      "Anger      1.0 It's got that about it.\n",
      "Happiness  1.0 That was a rouser, wasn't it?\n",
      "Sadness    1.0 Because this other half was just kind of like ripped away.\n",
      "Sadness    0.0 I mean, one second you think, you know, you're just having a conversation and then --\n",
      "Neutral    1.0 I know.  Okay.  Well, um. Maybe--\n",
      "Neutral    1.0 Do you want my jacket?\n",
      "Anger      0.0 Oh, oh, that's a good deduction.\n",
      "Anger      1.0 She's been in New York three and half years why all of a sudden-\n",
      "Happiness  0.0 What happens if one of us dies?  Does the one that's left, still laugh?\n",
      "Neutral    1.0 You were fine during the phone call from Marge, dinner was okay, doing the dishes was no problem, let's see, I took a shower, you watched the news, you said\n",
      "Neutral    1.0 Well, we can give you um- hotel lodging for a night if that--\n",
      "Happiness  0.0 I mean, think about this, I know you're not interested but just think about this for a minute.\n",
      "Sadness    1.0 He did a lot.  But--\n",
      "Happiness  1.0 Oh.  You knew there was nothing in that.\n",
      "Neutral    1.0 Um, I basically, um I- I ordered some food\n",
      "Happiness  0.0 Okay.  Is that, is that--\n",
      "Happiness  1.0 Yea, but that moment on the beach is the highlight of their little lives. I mean all that flopping is laying eggs or what do you, mating or spotting or something like that.\n",
      "Neutral    1.0 Just give it some more thought.\n",
      "Happiness  1.0 Yeah. I'm so excited. [LAUGHTER].\n",
      "Sadness    1.0 So talented.\n",
      "Neutral    1.0 What do you mean, dishonest?\n",
      "Happiness  1.0 You could make her that excited.  You really like her.  You guys have been together for a really long time now.\n",
      "Happiness  0.0 I knew they would, your mother anyway.\n",
      "Happiness  1.0 Augie, you brought refreshments.\n",
      "Anger      1.0 I am.  Let go of me.  You're a cruel fiend and I hate and loathe you.\n",
      "Happiness  1.0 But that- so we are really used to each other. We took the time apart and now we're going to get back together.  We're going to move in soon.\n",
      "Anger      1.0 I don't know why but every time I reach out for something that I really want, I have to pull back 'cause someone else will suffer. My whole bloody life, time after time after time.\n",
      "Happiness  1.0 We should try to find, like, a party spot though, like you know where there's going to be a lot of parties, where there are cool people, though.  Do you know any people that might be out there?\n",
      "Happiness  0.0 It's true.  The whole business is a very poor joke.\n",
      "Happiness  0.0 It's true.  The whole business is a very poor joke.\n",
      "Sadness    1.0 The National Guard is I guess really spread thin and then everyone has to go.  We all have to go back, my whole companies going back.\n",
      "Happiness  0.0 Oh, yeah? Have you gotten letters yet?\n",
      "Happiness  0.0 With perfect poise.\n",
      "Neutral    0.0 I wonder, do they always run during a full moon?  I can't remember if it was full last year.  Do you remember?\n",
      "Neutral    1.0 Ouch.  That's, um -- oh, wow.\n",
      "Happiness  1.0 Right.  Right.  Right. Oh, man, then you're going to have to do orientation, I guess.\n",
      "Anger      1.0 It's a pity you didn't have a little bit more brandy.  It might have made you a little less disagreeable.\n",
      "Happiness  1.0 I mean, it was, it's bound to come out sooner or later.\n",
      "Sadness    1.0 cause you can take those things out of the war.  But you got to know, when you drive that car that it came out of the love that a man can have for a man and you got to be a little bit better because of that.\n",
      "Neutral    1.0 I'm trying to be helpful.\n",
      "Sadness    1.0 But this isn't at all what I wanted it to be.\n",
      "Neutral    1.0 Um- It's a travel backpack.  It's green and gray.  It's got my name on it and my information.\n",
      "Happiness  1.0 A U.S.C. graduate now. You're set.\n",
      "Neutral    1.0 No.  Okay.  We're not charging you for any of these months.  We're going to give you three months free.\n",
      "Sadness    1.0 And I guess that included you.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m input_ids \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     13\u001b[0m     \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     output, attention \u001b[39m=\u001b[39m network(input_ids, sprectrome, output_attentions\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     15\u001b[0m     \u001b[39m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     _, preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(output, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/kuhaku/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Code/EmotionClassification/code/3m-ser-private/src/models/networks.py:146\u001b[0m, in \u001b[0;36mMMSERALayerNorm.forward\u001b[0;34m(self, input_ids, audio, output_attentions)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, audio, output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    145\u001b[0m     \u001b[39m# Text processing\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m     text_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(input_ids)\u001b[39m.\u001b[39mlast_hidden_state\n\u001b[1;32m    148\u001b[0m     \u001b[39m# Audio processing\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     audio_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvggish(audio)\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/kuhaku/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/kuhaku/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1011\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1013\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1014\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1015\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1019\u001b[0m )\n\u001b[0;32m-> 1020\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1021\u001b[0m     embedding_output,\n\u001b[1;32m   1022\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1023\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1024\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1025\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1026\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1027\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1028\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1029\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1030\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1031\u001b[0m )\n\u001b[1;32m   1032\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1033\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/kuhaku/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/kuhaku/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    608\u001b[0m     )\n\u001b[1;32m    609\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    611\u001b[0m         hidden_states,\n\u001b[1;32m    612\u001b[0m         attention_mask,\n\u001b[1;32m    613\u001b[0m         layer_head_mask,\n\u001b[1;32m    614\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    615\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    616\u001b[0m         past_key_value,\n\u001b[1;32m    617\u001b[0m         output_attentions,\n\u001b[1;32m    618\u001b[0m     )\n\u001b[1;32m    620\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    621\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/kuhaku/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/kuhaku/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:495\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    484\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    485\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    492\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    493\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    494\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    496\u001b[0m         hidden_states,\n\u001b[1;32m    497\u001b[0m         attention_mask,\n\u001b[1;32m    498\u001b[0m         head_mask,\n\u001b[1;32m    499\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    500\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    501\u001b[0m     )\n\u001b[1;32m    502\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    504\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/kuhaku/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/kuhaku/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:425\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    416\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    417\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    424\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 425\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    426\u001b[0m         hidden_states,\n\u001b[1;32m    427\u001b[0m         attention_mask,\n\u001b[1;32m    428\u001b[0m         head_mask,\n\u001b[1;32m    429\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    430\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    431\u001b[0m         past_key_value,\n\u001b[1;32m    432\u001b[0m         output_attentions,\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    434\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    435\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/kuhaku/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/kuhaku/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:306\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    304\u001b[0m     value_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m1\u001b[39m], value_layer], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    305\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 306\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey(hidden_states))\n\u001b[1;32m    307\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(hidden_states))\n\u001b[1;32m    309\u001b[0m query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/kuhaku/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/kuhaku/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "int2str = [\"Anger\", \"Happiness\", \"Sadness\", \"Neutral\"]\n",
    "for batch in test_ds:\n",
    "    # Prepare batch\n",
    "    text, label, sprectrome = batch[\"text\"], batch[\"label\"], batch[\"sprectrome\"]\n",
    "    label = torch.tensor([int(label)])\n",
    "    input_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0)\n",
    "\n",
    "    # Move inputs to cpu or gpu\n",
    "    sprectrome = sprectrome.to(device)\n",
    "    label = label.to(device)\n",
    "    input_ids = input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        output, attention = network(input_ids, sprectrome, output_attentions=True)\n",
    "        # Calculate accuracy\n",
    "        _, preds = torch.max(output, 1)\n",
    "        accuracy = torch.mean((preds == label).float())\n",
    "    if \"he did wriggle so beautifully\" in batch[\"text\"]:\n",
    "        print(\"{:<10} {} \".format(int2str[batch[\"label\"]], accuracy) + batch[\"text\"])\n",
    "    # if (int(accuracy) == 0):\n",
    "    #     print(batch[\"label\"]), print(preds)\n",
    "    #     head_view([attention[0]], tokenizer.tokenize('[CLS] '+ text + ' [SEP]'))\n",
    "    #     head_view([attention[1]], tokenizer.tokenize('[CLS] '+ text + ' [SEP]' + ''.join([' {}'.format(i) for i in range(sprectrome.shape[0])])))\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "      \n",
       "        <div id=\"bertviz-9ba0410cc25c4dfda02c6c49542556b9\" style=\"font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;\">\n",
       "            <span style=\"user-select:none\">\n",
       "                \n",
       "            </span>\n",
       "            <div id='vis'></div>\n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "/**\n * @fileoverview Transformer Visualization D3 javascript code.\n *\n * Based on: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/visualization/attention.js\n *\n * Change log:\n *\n * 02/01/19  Jesse Vig   Initial implementation\n * 12/31/20  Jesse Vig   Support multiple visualizations in single notebook.\n * 01/19/21  Jesse Vig   Support light/dark modes\n * 02/06/21  Jesse Vig   Move require config from separate jupyter notebook step\n * 05/03/21  Jesse Vig   Adjust visualization height dynamically\n * 03/23/22  Daniel SC   Update requirement URLs for d3 and jQuery (source of bug not allowing end result to be displayed on browsers)\n **/\n\nrequire.config({\n  paths: {\n      d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min',\n    jquery: 'https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.0/jquery.min',\n  }\n});\n\nrequirejs(['jquery', 'd3'], function($, d3) {\n\n        const params = {\"attention\": [{\"name\": null, \"attn\": [[[[0.0004335145349614322, 0.00048007795703597367, 0.00045686191879212856, 0.0007032177527435124, 0.000421658915001899, 0.0004903372027911246, 0.0003905182529706508, 0.5909806489944458, 0.14799782633781433, 0.2576453387737274], [0.00042805736302398145, 0.0004458686162251979, 0.00041286752093583345, 0.0006342388805933297, 0.00041806328226812184, 0.0004944797256030142, 0.00037616275949403644, 0.6495070457458496, 0.12421386688947678, 0.22306938469409943], [0.0005053805653005838, 0.0005123147857375443, 0.000466326717287302, 0.0007124551339074969, 0.0004896436585113406, 0.0005674692220054567, 0.00043002533493563533, 0.6374155879020691, 0.13052178919315338, 0.22837893664836884], [0.0007126444252207875, 0.0007013918366283178, 0.0006457131821662188, 0.0009336029179394245, 0.0007484792149625719, 0.0008699434692971408, 0.0006675586919300258, 0.6550513505935669, 0.11960664391517639, 0.22006267309188843], [0.0009235545294359326, 0.0010641340631991625, 0.0010012042475864291, 0.001532448222860694, 0.000811733421869576, 0.0008719577454030514, 0.0007376907160505652, 0.5416621565818787, 0.17223027348518372, 0.2791648209095001], [0.002361444290727377, 0.002846725285053253, 0.0027691724244505167, 0.00398928951472044, 0.002020705956965685, 0.001987113617360592, 0.0018935304833576083, 0.45962971448898315, 0.1917046159505844, 0.33079764246940613], [0.0012427275069057941, 0.0013962768716737628, 0.0012976237339898944, 0.0019529220880940557, 0.0010819380404427648, 0.0011168916244059801, 0.0009623837540857494, 0.5035586357116699, 0.19004589319229126, 0.2973446547985077], [0.08923109620809555, 0.08672583848237991, 0.09560777246952057, 0.07948733866214752, 0.14046157896518707, 0.14776507019996643, 0.15837304294109344, 0.04619069769978523, 0.08441844582557678, 0.07173909991979599], [0.08825026452541351, 0.08041057735681534, 0.08928607404232025, 0.06872190535068512, 0.16165408492088318, 0.16621555387973785, 0.18246982991695404, 0.0372387133538723, 0.06866024434566498, 0.05709284916520119], [0.08991140872240067, 0.07888268679380417, 0.08387789130210876, 0.06920364499092102, 0.14043129980564117, 0.15619948506355286, 0.1520918905735016, 0.05293824151158333, 0.10981666296720505, 0.06664681434631348]], [[0.00012374001380521804, 0.0001333524560322985, 0.00015394159709103405, 0.00021591140830423683, 0.00031516802846454084, 0.0004191158222965896, 0.0003684459370560944, 0.17512263357639313, 0.7263480424880981, 0.09679965674877167], [0.00015619733312632889, 0.00017174941604025662, 0.0001957455970114097, 0.00028694147476926446, 0.00033937476109713316, 0.00045980195864103734, 0.00039514139643870294, 0.18123270571231842, 0.7250888347625732, 0.09167347103357315], [0.0001933989697135985, 0.00021378854580689222, 0.0002431253087706864, 0.00036190878017805517, 0.00038211027276702225, 0.0005262134945951402, 0.0004476875183172524, 0.18082404136657715, 0.7297983765602112, 0.08700945973396301], [0.00015028864436317235, 0.0001651171041885391, 0.0001916662004077807, 0.00027860046247951686, 0.0003228848800063133, 0.00044061135849915445, 0.0003837352560367435, 0.16607946157455444, 0.7434977293014526, 0.08848987519741058], [0.0013196334475651383, 0.0014278413727879524, 0.0015110955573618412, 0.002151534426957369, 0.002052138326689601, 0.0026435062754899263, 0.0022473244462162256, 0.25720348954200745, 0.6068896055221558, 0.12255392223596573], [0.0004880481865257025, 0.0005165766342543066, 0.0005885059363208711, 0.000788274803198874, 0.000996912713162601, 0.0013548079878091812, 0.0011801067739725113, 0.21585719287395477, 0.6709643006324768, 0.10726514458656311], [0.0012338417582213879, 0.001342911389656365, 0.001446629874408245, 0.0020881611853837967, 0.001820801175199449, 0.0024344874545931816, 0.0020530757028609514, 0.24410806596279144, 0.6340407729148865, 0.10943131893873215], [0.11195892095565796, 0.10530772060155869, 0.10618666559457779, 0.09634477645158768, 0.1326592117547989, 0.1181502416729927, 0.13071857392787933, 0.05344516038894653, 0.05733536183834076, 0.08789337426424026], [0.16548138856887817, 0.1505485326051712, 0.13790886104106903, 0.11676584184169769, 0.14435236155986786, 0.11368103325366974, 0.12800349295139313, 0.010920251719653606, 0.009379507042467594, 0.022958694025874138], [0.14346584677696228, 0.1342325508594513, 0.12899845838546753, 0.11444149911403656, 0.13944581151008606, 0.11379451304674149, 0.1287262886762619, 0.03539755940437317, 0.027113528922200203, 0.034383974969387054]], [[0.00031552306609228253, 0.00028910196851938963, 0.00027555826818570495, 0.0005241454346105456, 0.00019307313777972013, 0.0006087521323934197, 0.00020182896696496755, 0.09114882349967957, 0.6911482810974121, 0.21529492735862732], [0.00030702154617756605, 0.0002857994695659727, 0.00027764635160565376, 0.0005384900723583996, 0.00018020487914327532, 0.0005897515802644193, 0.00019574658654164523, 0.07284629344940186, 0.7407283186912537, 0.18405073881149292], [0.0004356481076683849, 0.00040582535439170897, 0.0003931294195353985, 0.0007503944216296077, 0.0002498415415175259, 0.0007967075798660517, 0.0002693248388823122, 0.06567572057247162, 0.7519554495811462, 0.17906804382801056], [0.0004120043886359781, 0.00038992895861156285, 0.0003668637073133141, 0.0006919834413565695, 0.00022933806758373976, 0.0006870781653560698, 0.00023453585163224488, 0.05406159535050392, 0.7331315875053406, 0.20979511737823486], [0.002712665358558297, 0.002392873400822282, 0.00241559324786067, 0.004052693955600262, 0.0019611683674156666, 0.005642573349177837, 0.002253769664093852, 0.2200268805027008, 0.5809630751609802, 0.17757856845855713], [0.008403451181948185, 0.0074804131872951984, 0.006965699139982462, 0.010867497883737087, 0.005556316114962101, 0.012997838668525219, 0.005565931089222431, 0.2202611267566681, 0.43420419096946716, 0.28769755363464355], [0.0046339561231434345, 0.004107437562197447, 0.004044692497700453, 0.006618991959840059, 0.003136520506814122, 0.008411633782088757, 0.0034387477207928896, 0.18202278017997742, 0.5888673663139343, 0.19471785426139832], [0.1183423176407814, 0.10740721225738525, 0.0963815450668335, 0.09011343866586685, 0.13176433742046356, 0.12668029963970184, 0.1132771372795105, 0.07273431867361069, 0.04529586806893349, 0.09800352156162262], [0.08316896110773087, 0.07634342461824417, 0.06911066919565201, 0.0619933046400547, 0.10134943574666977, 0.09381591528654099, 0.08571648597717285, 0.1623762845993042, 0.07860440760850906, 0.18752111494541168], [0.10273783653974533, 0.09946358948945999, 0.09651673585176468, 0.0878995805978775, 0.1315382868051529, 0.12953323125839233, 0.12450863420963287, 0.06753107160329819, 0.0656059980392456, 0.09466501325368881]], [[0.006758902687579393, 0.006115941796451807, 0.0062686423771083355, 0.006751686800271273, 0.010212024673819542, 0.012940001673996449, 0.011519279330968857, 0.4166072905063629, 0.3227387070655823, 0.20008747279644012], [0.006212606094777584, 0.005687079392373562, 0.005819088779389858, 0.006305447779595852, 0.009215498343110085, 0.011656299233436584, 0.010442428290843964, 0.45020994544029236, 0.2994005084037781, 0.19505111873149872], [0.006147258915007114, 0.00586040411144495, 0.005930228158831596, 0.006594470236450434, 0.008860751986503601, 0.010947699658572674, 0.009810179471969604, 0.514317512512207, 0.24943244457244873, 0.18209907412528992], [0.006360452622175217, 0.006421753205358982, 0.006552811712026596, 0.007106076925992966, 0.010039876215159893, 0.01156772393733263, 0.010950850322842598, 0.5386062264442444, 0.21049928665161133, 0.19189493358135223], [0.012038478627800941, 0.00915079191327095, 0.009114786051213741, 0.01068758126348257, 0.011956955306231976, 0.018277425318956375, 0.013872990384697914, 0.2646976411342621, 0.503485381603241, 0.14671793580055237], [0.01517120748758316, 0.012105632573366165, 0.011874480172991753, 0.014561403542757034, 0.014536972157657146, 0.02195383794605732, 0.01635715924203396, 0.2782785892486572, 0.4774881899356842, 0.13767246901988983], [0.0127619169652462, 0.010538682341575623, 0.010354687459766865, 0.012554132379591465, 0.012601804919540882, 0.018185963854193687, 0.014026970602571964, 0.360088974237442, 0.40369871258735657, 0.1451881229877472], [0.12038951367139816, 0.13140124082565308, 0.13038553297519684, 0.13412895798683167, 0.10643454641103745, 0.09323062747716904, 0.09871247410774231, 0.06547095626592636, 0.0650947242975235, 0.05475145950913429], [0.11323317885398865, 0.13678686320781708, 0.1311248540878296, 0.13245174288749695, 0.10409750789403915, 0.08224361389875412, 0.09004735946655273, 0.08394533395767212, 0.03650480881333351, 0.08956468850374222], [0.1262863129377365, 0.14201965928077698, 0.13897652924060822, 0.13906052708625793, 0.1117374449968338, 0.09375125169754028, 0.10082143545150757, 0.050141993910074234, 0.04258979484438896, 0.0546150803565979]], [[0.0019665230065584183, 0.002067540306597948, 0.0023570586927235126, 0.0025173304602503777, 0.006366817746311426, 0.013186234049499035, 0.008513875305652618, 0.17599846422672272, 0.758073627948761, 0.028952552005648613], [0.0017493151826784015, 0.001800922560505569, 0.0020750355906784534, 0.0021918448619544506, 0.005732256453484297, 0.011485935188829899, 0.0077409823425114155, 0.17852745950222015, 0.7581806182861328, 0.03051559254527092], [0.002191560110077262, 0.0022569827269762754, 0.0025790526997298002, 0.002698486903682351, 0.007004327140748501, 0.013518031686544418, 0.009257583878934383, 0.19289295375347137, 0.7322834730148315, 0.03531758487224579], [0.0017692429246380925, 0.0018484321190044284, 0.0021405646111816168, 0.0022398270666599274, 0.006033123470842838, 0.011723332107067108, 0.008066904731094837, 0.19502167403697968, 0.7367780208587646, 0.034379005432128906], [0.008076686412096024, 0.008005308918654919, 0.008781397715210915, 0.009175846353173256, 0.01752946339547634, 0.030347568914294243, 0.021935466676950455, 0.20454835891723633, 0.6432170271873474, 0.0483829565346241], [0.007575458846986294, 0.007714556995779276, 0.00863358099013567, 0.00891927257180214, 0.016819966956973076, 0.028624337166547775, 0.02116331271827221, 0.24381481111049652, 0.6085711717605591, 0.048163630068302155], [0.008893377147614956, 0.008962753228843212, 0.009795467369258404, 0.010094262659549713, 0.019812852144241333, 0.033436309546232224, 0.02427070029079914, 0.2276517003774643, 0.6047043800354004, 0.05237823724746704], [0.11164388805627823, 0.11695550382137299, 0.1187095046043396, 0.11757902055978775, 0.11059095710515976, 0.09493998438119888, 0.10768062621355057, 0.06879094988107681, 0.06651864945888519, 0.08659092336893082], [0.0832839235663414, 0.0883701741695404, 0.08936860412359238, 0.08605259656906128, 0.10427235066890717, 0.10670209676027298, 0.1014481708407402, 0.13591593503952026, 0.10179772228002548, 0.10278832167387009], [0.1230575367808342, 0.12035736441612244, 0.1177806630730629, 0.11612262576818466, 0.1108790710568428, 0.09800955653190613, 0.10591422021389008, 0.055317651480436325, 0.061452072113752365, 0.09110917150974274]], [[0.0001443796936655417, 5.105251329950988e-05, 3.08863163809292e-05, 2.437834336888045e-05, 0.0009406966273672879, 0.001747951959259808, 0.0004784947959706187, 0.33648598194122314, 0.23581209778785706, 0.4242841303348541], [0.00020326868980191648, 7.005129737081006e-05, 4.1170231270371005e-05, 3.360222763149068e-05, 0.0010852681007236242, 0.0020315058063715696, 0.0005420294473879039, 0.3494722843170166, 0.22164344787597656, 0.4248773753643036], [0.00017512071644887328, 5.864421837031841e-05, 3.29881913785357e-05, 2.8046768420608714e-05, 0.000800146721303463, 0.0014266361249610782, 0.00037566301762126386, 0.33455416560173035, 0.2053244560956955, 0.45722419023513794], [0.00019369165238458663, 7.071273284964263e-05, 4.181308395345695e-05, 3.356502566020936e-05, 0.000983063131570816, 0.0015249964781105518, 0.0004708972410298884, 0.3277426064014435, 0.19111977517604828, 0.4778188467025757], [0.0006171464337967336, 0.00019860865722876042, 0.00010992233001161367, 0.0001170352625194937, 0.0014915087958797812, 0.003768710885196924, 0.0007993729668669403, 0.32119888067245483, 0.3321399390697479, 0.33955878019332886], [0.0003604436351452023, 0.00012298145156819373, 7.045130769256502e-05, 7.055417518131435e-05, 0.0010849613463506103, 0.0022927147801965475, 0.0005645747296512127, 0.2716697156429291, 0.3215547204017639, 0.4022088646888733], [0.000391085835872218, 0.00012249589781276882, 6.445532198995352e-05, 7.03017576597631e-05, 0.0008559999987483025, 0.001915999106131494, 0.00041573541238904, 0.29729360342025757, 0.29469284415245056, 0.4041775166988373], [0.09191259741783142, 0.08351436257362366, 0.07322327047586441, 0.08060788363218307, 0.07655268907546997, 0.0627940446138382, 0.06155790761113167, 0.10156989097595215, 0.20783871412277222, 0.16042856872081757], [0.08607351034879684, 0.09804399311542511, 0.09383340179920197, 0.09673906862735748, 0.08713958412408829, 0.06651749461889267, 0.07551290839910507, 0.0997978001832962, 0.14584404230117798, 0.1504981964826584], [0.1167965903878212, 0.1262742280960083, 0.12416408956050873, 0.12934757769107819, 0.09695128351449966, 0.08322317153215408, 0.09207528084516525, 0.06812585890293121, 0.07721817493438721, 0.08582375198602676]], [[0.06174463406205177, 0.060343705117702484, 0.05532621219754219, 0.0556923970580101, 0.10953131318092346, 0.10478250682353973, 0.1025942862033844, 0.11701005697250366, 0.26477932929992676, 0.06819550693035126], [0.05737527459859848, 0.05641238018870354, 0.05182083696126938, 0.052378397434949875, 0.10232412070035934, 0.09790851175785065, 0.09524307399988174, 0.14876671135425568, 0.25584524869918823, 0.08192548900842667], [0.056446269154548645, 0.057736240327358246, 0.05309578403830528, 0.05416083335876465, 0.105221688747406, 0.0965055301785469, 0.09631180018186569, 0.17205199599266052, 0.22485582530498505, 0.08361407369375229], [0.06137697771191597, 0.06352630257606506, 0.05790478363633156, 0.06034360080957413, 0.10022950917482376, 0.09571260213851929, 0.09038197994232178, 0.17438551783561707, 0.20218704640865326, 0.09395177662372589], [0.04159977287054062, 0.037661295384168625, 0.03745940327644348, 0.03511211276054382, 0.09710542112588882, 0.0827965959906578, 0.10113614797592163, 0.12683972716331482, 0.36980167031288147, 0.07048781961202621], [0.041782304644584656, 0.039417847990989685, 0.03946101292967796, 0.03749019652605057, 0.10062824934720993, 0.08213426172733307, 0.10398875921964645, 0.1453200727701187, 0.3358392119407654, 0.07393816113471985], [0.04338971897959709, 0.0421336330473423, 0.041615571826696396, 0.039919834583997726, 0.10604841262102127, 0.08565715700387955, 0.10669252276420593, 0.15822292864322662, 0.3052745461463928, 0.0710456594824791], [0.0882025957107544, 0.10045292228460312, 0.10461178421974182, 0.11445561796426773, 0.06722372025251389, 0.056006789207458496, 0.06394340097904205, 0.10667425394058228, 0.08378022909164429, 0.21464863419532776], [0.0905182883143425, 0.09965719282627106, 0.10822823643684387, 0.10494418442249298, 0.09079543501138687, 0.08500868827104568, 0.09475062042474747, 0.08381133526563644, 0.10839370638132095, 0.1338922381401062], [0.09909325838088989, 0.10919849574565887, 0.11253230273723602, 0.1162782534956932, 0.07863851636648178, 0.0742821916937828, 0.07523003965616226, 0.08783216774463654, 0.08086008578538895, 0.16605475544929504]], [[0.10777217894792557, 0.1164829358458519, 0.10241525620222092, 0.1048697680234909, 0.12877261638641357, 0.10288788378238678, 0.09608875215053558, 0.13572509586811066, 0.016468998044729233, 0.0885164737701416], [0.10627574473619461, 0.11028560996055603, 0.09681695699691772, 0.09508325904607773, 0.14073830842971802, 0.11380008608102798, 0.10413898527622223, 0.1326913833618164, 0.01598973385989666, 0.08417993783950806], [0.10404772311449051, 0.10812189429998398, 0.09558924287557602, 0.09452077001333237, 0.13566812872886658, 0.11064371466636658, 0.10205669701099396, 0.14470800757408142, 0.020212125033140182, 0.08443168550729752], [0.1060829907655716, 0.11171060800552368, 0.10172685235738754, 0.10015898197889328, 0.13358788192272186, 0.1137285828590393, 0.10637165606021881, 0.13096174597740173, 0.023272953927516937, 0.07239770144224167], [0.09440205991268158, 0.09415507316589355, 0.07935306429862976, 0.07983926683664322, 0.12613163888454437, 0.0951763391494751, 0.086186982691288, 0.1758033037185669, 0.01772117055952549, 0.15123111009597778], [0.09082901477813721, 0.10592064261436462, 0.0950343981385231, 0.10147632658481598, 0.09894976019859314, 0.07544983178377151, 0.07574857026338577, 0.19657862186431885, 0.017011739313602448, 0.14300116896629333], [0.09111660718917847, 0.09459277987480164, 0.08165676146745682, 0.08444364368915558, 0.1124664694070816, 0.08626364916563034, 0.08076751977205276, 0.1983431875705719, 0.0244691651314497, 0.14588023722171783], [0.08582326024770737, 0.10099340230226517, 0.11163731664419174, 0.1302126795053482, 0.05722002312541008, 0.05987083911895752, 0.067438043653965, 0.1265861988067627, 0.1594693511724472, 0.10074880719184875], [0.08661941438913345, 0.09636399894952774, 0.10895460098981857, 0.11731579899787903, 0.060275848954916, 0.06652932614088058, 0.07576152682304382, 0.13266365230083466, 0.15092024207115173, 0.1045956090092659], [0.11494489759206772, 0.11919458955526352, 0.1282210499048233, 0.13655510544776917, 0.08697447180747986, 0.09641673415899277, 0.09984277188777924, 0.06288330256938934, 0.09081854671239853, 0.06414851546287537]]]], \"left_text\": [\"[CLS]\", \"my\", \"name\", \"is\", \"sara\", \".\", \"[SEP]\", \"0\", \"1\", \"2\"], \"right_text\": [\"[CLS]\", \"my\", \"name\", \"is\", \"sara\", \".\", \"[SEP]\", \"0\", \"1\", \"2\"]}], \"default_filter\": \"0\", \"display_mode\": \"dark\", \"root_div_id\": \"bertviz-9ba0410cc25c4dfda02c6c49542556b9\", \"include_layers\": [0], \"include_heads\": [0, 1, 2, 3, 4, 5, 6, 7], \"total_heads\": 8}; // HACK: {\"attention\": [{\"name\": null, \"attn\": [[[[0.0004335145349614322, 0.00048007795703597367, 0.00045686191879212856, 0.0007032177527435124, 0.000421658915001899, 0.0004903372027911246, 0.0003905182529706508, 0.5909806489944458, 0.14799782633781433, 0.2576453387737274], [0.00042805736302398145, 0.0004458686162251979, 0.00041286752093583345, 0.0006342388805933297, 0.00041806328226812184, 0.0004944797256030142, 0.00037616275949403644, 0.6495070457458496, 0.12421386688947678, 0.22306938469409943], [0.0005053805653005838, 0.0005123147857375443, 0.000466326717287302, 0.0007124551339074969, 0.0004896436585113406, 0.0005674692220054567, 0.00043002533493563533, 0.6374155879020691, 0.13052178919315338, 0.22837893664836884], [0.0007126444252207875, 0.0007013918366283178, 0.0006457131821662188, 0.0009336029179394245, 0.0007484792149625719, 0.0008699434692971408, 0.0006675586919300258, 0.6550513505935669, 0.11960664391517639, 0.22006267309188843], [0.0009235545294359326, 0.0010641340631991625, 0.0010012042475864291, 0.001532448222860694, 0.000811733421869576, 0.0008719577454030514, 0.0007376907160505652, 0.5416621565818787, 0.17223027348518372, 0.2791648209095001], [0.002361444290727377, 0.002846725285053253, 0.0027691724244505167, 0.00398928951472044, 0.002020705956965685, 0.001987113617360592, 0.0018935304833576083, 0.45962971448898315, 0.1917046159505844, 0.33079764246940613], [0.0012427275069057941, 0.0013962768716737628, 0.0012976237339898944, 0.0019529220880940557, 0.0010819380404427648, 0.0011168916244059801, 0.0009623837540857494, 0.5035586357116699, 0.19004589319229126, 0.2973446547985077], [0.08923109620809555, 0.08672583848237991, 0.09560777246952057, 0.07948733866214752, 0.14046157896518707, 0.14776507019996643, 0.15837304294109344, 0.04619069769978523, 0.08441844582557678, 0.07173909991979599], [0.08825026452541351, 0.08041057735681534, 0.08928607404232025, 0.06872190535068512, 0.16165408492088318, 0.16621555387973785, 0.18246982991695404, 0.0372387133538723, 0.06866024434566498, 0.05709284916520119], [0.08991140872240067, 0.07888268679380417, 0.08387789130210876, 0.06920364499092102, 0.14043129980564117, 0.15619948506355286, 0.1520918905735016, 0.05293824151158333, 0.10981666296720505, 0.06664681434631348]], [[0.00012374001380521804, 0.0001333524560322985, 0.00015394159709103405, 0.00021591140830423683, 0.00031516802846454084, 0.0004191158222965896, 0.0003684459370560944, 0.17512263357639313, 0.7263480424880981, 0.09679965674877167], [0.00015619733312632889, 0.00017174941604025662, 0.0001957455970114097, 0.00028694147476926446, 0.00033937476109713316, 0.00045980195864103734, 0.00039514139643870294, 0.18123270571231842, 0.7250888347625732, 0.09167347103357315], [0.0001933989697135985, 0.00021378854580689222, 0.0002431253087706864, 0.00036190878017805517, 0.00038211027276702225, 0.0005262134945951402, 0.0004476875183172524, 0.18082404136657715, 0.7297983765602112, 0.08700945973396301], [0.00015028864436317235, 0.0001651171041885391, 0.0001916662004077807, 0.00027860046247951686, 0.0003228848800063133, 0.00044061135849915445, 0.0003837352560367435, 0.16607946157455444, 0.7434977293014526, 0.08848987519741058], [0.0013196334475651383, 0.0014278413727879524, 0.0015110955573618412, 0.002151534426957369, 0.002052138326689601, 0.0026435062754899263, 0.0022473244462162256, 0.25720348954200745, 0.6068896055221558, 0.12255392223596573], [0.0004880481865257025, 0.0005165766342543066, 0.0005885059363208711, 0.000788274803198874, 0.000996912713162601, 0.0013548079878091812, 0.0011801067739725113, 0.21585719287395477, 0.6709643006324768, 0.10726514458656311], [0.0012338417582213879, 0.001342911389656365, 0.001446629874408245, 0.0020881611853837967, 0.001820801175199449, 0.0024344874545931816, 0.0020530757028609514, 0.24410806596279144, 0.6340407729148865, 0.10943131893873215], [0.11195892095565796, 0.10530772060155869, 0.10618666559457779, 0.09634477645158768, 0.1326592117547989, 0.1181502416729927, 0.13071857392787933, 0.05344516038894653, 0.05733536183834076, 0.08789337426424026], [0.16548138856887817, 0.1505485326051712, 0.13790886104106903, 0.11676584184169769, 0.14435236155986786, 0.11368103325366974, 0.12800349295139313, 0.010920251719653606, 0.009379507042467594, 0.022958694025874138], [0.14346584677696228, 0.1342325508594513, 0.12899845838546753, 0.11444149911403656, 0.13944581151008606, 0.11379451304674149, 0.1287262886762619, 0.03539755940437317, 0.027113528922200203, 0.034383974969387054]], [[0.00031552306609228253, 0.00028910196851938963, 0.00027555826818570495, 0.0005241454346105456, 0.00019307313777972013, 0.0006087521323934197, 0.00020182896696496755, 0.09114882349967957, 0.6911482810974121, 0.21529492735862732], [0.00030702154617756605, 0.0002857994695659727, 0.00027764635160565376, 0.0005384900723583996, 0.00018020487914327532, 0.0005897515802644193, 0.00019574658654164523, 0.07284629344940186, 0.7407283186912537, 0.18405073881149292], [0.0004356481076683849, 0.00040582535439170897, 0.0003931294195353985, 0.0007503944216296077, 0.0002498415415175259, 0.0007967075798660517, 0.0002693248388823122, 0.06567572057247162, 0.7519554495811462, 0.17906804382801056], [0.0004120043886359781, 0.00038992895861156285, 0.0003668637073133141, 0.0006919834413565695, 0.00022933806758373976, 0.0006870781653560698, 0.00023453585163224488, 0.05406159535050392, 0.7331315875053406, 0.20979511737823486], [0.002712665358558297, 0.002392873400822282, 0.00241559324786067, 0.004052693955600262, 0.0019611683674156666, 0.005642573349177837, 0.002253769664093852, 0.2200268805027008, 0.5809630751609802, 0.17757856845855713], [0.008403451181948185, 0.0074804131872951984, 0.006965699139982462, 0.010867497883737087, 0.005556316114962101, 0.012997838668525219, 0.005565931089222431, 0.2202611267566681, 0.43420419096946716, 0.28769755363464355], [0.0046339561231434345, 0.004107437562197447, 0.004044692497700453, 0.006618991959840059, 0.003136520506814122, 0.008411633782088757, 0.0034387477207928896, 0.18202278017997742, 0.5888673663139343, 0.19471785426139832], [0.1183423176407814, 0.10740721225738525, 0.0963815450668335, 0.09011343866586685, 0.13176433742046356, 0.12668029963970184, 0.1132771372795105, 0.07273431867361069, 0.04529586806893349, 0.09800352156162262], [0.08316896110773087, 0.07634342461824417, 0.06911066919565201, 0.0619933046400547, 0.10134943574666977, 0.09381591528654099, 0.08571648597717285, 0.1623762845993042, 0.07860440760850906, 0.18752111494541168], [0.10273783653974533, 0.09946358948945999, 0.09651673585176468, 0.0878995805978775, 0.1315382868051529, 0.12953323125839233, 0.12450863420963287, 0.06753107160329819, 0.0656059980392456, 0.09466501325368881]], [[0.006758902687579393, 0.006115941796451807, 0.0062686423771083355, 0.006751686800271273, 0.010212024673819542, 0.012940001673996449, 0.011519279330968857, 0.4166072905063629, 0.3227387070655823, 0.20008747279644012], [0.006212606094777584, 0.005687079392373562, 0.005819088779389858, 0.006305447779595852, 0.009215498343110085, 0.011656299233436584, 0.010442428290843964, 0.45020994544029236, 0.2994005084037781, 0.19505111873149872], [0.006147258915007114, 0.00586040411144495, 0.005930228158831596, 0.006594470236450434, 0.008860751986503601, 0.010947699658572674, 0.009810179471969604, 0.514317512512207, 0.24943244457244873, 0.18209907412528992], [0.006360452622175217, 0.006421753205358982, 0.006552811712026596, 0.007106076925992966, 0.010039876215159893, 0.01156772393733263, 0.010950850322842598, 0.5386062264442444, 0.21049928665161133, 0.19189493358135223], [0.012038478627800941, 0.00915079191327095, 0.009114786051213741, 0.01068758126348257, 0.011956955306231976, 0.018277425318956375, 0.013872990384697914, 0.2646976411342621, 0.503485381603241, 0.14671793580055237], [0.01517120748758316, 0.012105632573366165, 0.011874480172991753, 0.014561403542757034, 0.014536972157657146, 0.02195383794605732, 0.01635715924203396, 0.2782785892486572, 0.4774881899356842, 0.13767246901988983], [0.0127619169652462, 0.010538682341575623, 0.010354687459766865, 0.012554132379591465, 0.012601804919540882, 0.018185963854193687, 0.014026970602571964, 0.360088974237442, 0.40369871258735657, 0.1451881229877472], [0.12038951367139816, 0.13140124082565308, 0.13038553297519684, 0.13412895798683167, 0.10643454641103745, 0.09323062747716904, 0.09871247410774231, 0.06547095626592636, 0.0650947242975235, 0.05475145950913429], [0.11323317885398865, 0.13678686320781708, 0.1311248540878296, 0.13245174288749695, 0.10409750789403915, 0.08224361389875412, 0.09004735946655273, 0.08394533395767212, 0.03650480881333351, 0.08956468850374222], [0.1262863129377365, 0.14201965928077698, 0.13897652924060822, 0.13906052708625793, 0.1117374449968338, 0.09375125169754028, 0.10082143545150757, 0.050141993910074234, 0.04258979484438896, 0.0546150803565979]], [[0.0019665230065584183, 0.002067540306597948, 0.0023570586927235126, 0.0025173304602503777, 0.006366817746311426, 0.013186234049499035, 0.008513875305652618, 0.17599846422672272, 0.758073627948761, 0.028952552005648613], [0.0017493151826784015, 0.001800922560505569, 0.0020750355906784534, 0.0021918448619544506, 0.005732256453484297, 0.011485935188829899, 0.0077409823425114155, 0.17852745950222015, 0.7581806182861328, 0.03051559254527092], [0.002191560110077262, 0.0022569827269762754, 0.0025790526997298002, 0.002698486903682351, 0.007004327140748501, 0.013518031686544418, 0.009257583878934383, 0.19289295375347137, 0.7322834730148315, 0.03531758487224579], [0.0017692429246380925, 0.0018484321190044284, 0.0021405646111816168, 0.0022398270666599274, 0.006033123470842838, 0.011723332107067108, 0.008066904731094837, 0.19502167403697968, 0.7367780208587646, 0.034379005432128906], [0.008076686412096024, 0.008005308918654919, 0.008781397715210915, 0.009175846353173256, 0.01752946339547634, 0.030347568914294243, 0.021935466676950455, 0.20454835891723633, 0.6432170271873474, 0.0483829565346241], [0.007575458846986294, 0.007714556995779276, 0.00863358099013567, 0.00891927257180214, 0.016819966956973076, 0.028624337166547775, 0.02116331271827221, 0.24381481111049652, 0.6085711717605591, 0.048163630068302155], [0.008893377147614956, 0.008962753228843212, 0.009795467369258404, 0.010094262659549713, 0.019812852144241333, 0.033436309546232224, 0.02427070029079914, 0.2276517003774643, 0.6047043800354004, 0.05237823724746704], [0.11164388805627823, 0.11695550382137299, 0.1187095046043396, 0.11757902055978775, 0.11059095710515976, 0.09493998438119888, 0.10768062621355057, 0.06879094988107681, 0.06651864945888519, 0.08659092336893082], [0.0832839235663414, 0.0883701741695404, 0.08936860412359238, 0.08605259656906128, 0.10427235066890717, 0.10670209676027298, 0.1014481708407402, 0.13591593503952026, 0.10179772228002548, 0.10278832167387009], [0.1230575367808342, 0.12035736441612244, 0.1177806630730629, 0.11612262576818466, 0.1108790710568428, 0.09800955653190613, 0.10591422021389008, 0.055317651480436325, 0.061452072113752365, 0.09110917150974274]], [[0.0001443796936655417, 5.105251329950988e-05, 3.08863163809292e-05, 2.437834336888045e-05, 0.0009406966273672879, 0.001747951959259808, 0.0004784947959706187, 0.33648598194122314, 0.23581209778785706, 0.4242841303348541], [0.00020326868980191648, 7.005129737081006e-05, 4.1170231270371005e-05, 3.360222763149068e-05, 0.0010852681007236242, 0.0020315058063715696, 0.0005420294473879039, 0.3494722843170166, 0.22164344787597656, 0.4248773753643036], [0.00017512071644887328, 5.864421837031841e-05, 3.29881913785357e-05, 2.8046768420608714e-05, 0.000800146721303463, 0.0014266361249610782, 0.00037566301762126386, 0.33455416560173035, 0.2053244560956955, 0.45722419023513794], [0.00019369165238458663, 7.071273284964263e-05, 4.181308395345695e-05, 3.356502566020936e-05, 0.000983063131570816, 0.0015249964781105518, 0.0004708972410298884, 0.3277426064014435, 0.19111977517604828, 0.4778188467025757], [0.0006171464337967336, 0.00019860865722876042, 0.00010992233001161367, 0.0001170352625194937, 0.0014915087958797812, 0.003768710885196924, 0.0007993729668669403, 0.32119888067245483, 0.3321399390697479, 0.33955878019332886], [0.0003604436351452023, 0.00012298145156819373, 7.045130769256502e-05, 7.055417518131435e-05, 0.0010849613463506103, 0.0022927147801965475, 0.0005645747296512127, 0.2716697156429291, 0.3215547204017639, 0.4022088646888733], [0.000391085835872218, 0.00012249589781276882, 6.445532198995352e-05, 7.03017576597631e-05, 0.0008559999987483025, 0.001915999106131494, 0.00041573541238904, 0.29729360342025757, 0.29469284415245056, 0.4041775166988373], [0.09191259741783142, 0.08351436257362366, 0.07322327047586441, 0.08060788363218307, 0.07655268907546997, 0.0627940446138382, 0.06155790761113167, 0.10156989097595215, 0.20783871412277222, 0.16042856872081757], [0.08607351034879684, 0.09804399311542511, 0.09383340179920197, 0.09673906862735748, 0.08713958412408829, 0.06651749461889267, 0.07551290839910507, 0.0997978001832962, 0.14584404230117798, 0.1504981964826584], [0.1167965903878212, 0.1262742280960083, 0.12416408956050873, 0.12934757769107819, 0.09695128351449966, 0.08322317153215408, 0.09207528084516525, 0.06812585890293121, 0.07721817493438721, 0.08582375198602676]], [[0.06174463406205177, 0.060343705117702484, 0.05532621219754219, 0.0556923970580101, 0.10953131318092346, 0.10478250682353973, 0.1025942862033844, 0.11701005697250366, 0.26477932929992676, 0.06819550693035126], [0.05737527459859848, 0.05641238018870354, 0.05182083696126938, 0.052378397434949875, 0.10232412070035934, 0.09790851175785065, 0.09524307399988174, 0.14876671135425568, 0.25584524869918823, 0.08192548900842667], [0.056446269154548645, 0.057736240327358246, 0.05309578403830528, 0.05416083335876465, 0.105221688747406, 0.0965055301785469, 0.09631180018186569, 0.17205199599266052, 0.22485582530498505, 0.08361407369375229], [0.06137697771191597, 0.06352630257606506, 0.05790478363633156, 0.06034360080957413, 0.10022950917482376, 0.09571260213851929, 0.09038197994232178, 0.17438551783561707, 0.20218704640865326, 0.09395177662372589], [0.04159977287054062, 0.037661295384168625, 0.03745940327644348, 0.03511211276054382, 0.09710542112588882, 0.0827965959906578, 0.10113614797592163, 0.12683972716331482, 0.36980167031288147, 0.07048781961202621], [0.041782304644584656, 0.039417847990989685, 0.03946101292967796, 0.03749019652605057, 0.10062824934720993, 0.08213426172733307, 0.10398875921964645, 0.1453200727701187, 0.3358392119407654, 0.07393816113471985], [0.04338971897959709, 0.0421336330473423, 0.041615571826696396, 0.039919834583997726, 0.10604841262102127, 0.08565715700387955, 0.10669252276420593, 0.15822292864322662, 0.3052745461463928, 0.0710456594824791], [0.0882025957107544, 0.10045292228460312, 0.10461178421974182, 0.11445561796426773, 0.06722372025251389, 0.056006789207458496, 0.06394340097904205, 0.10667425394058228, 0.08378022909164429, 0.21464863419532776], [0.0905182883143425, 0.09965719282627106, 0.10822823643684387, 0.10494418442249298, 0.09079543501138687, 0.08500868827104568, 0.09475062042474747, 0.08381133526563644, 0.10839370638132095, 0.1338922381401062], [0.09909325838088989, 0.10919849574565887, 0.11253230273723602, 0.1162782534956932, 0.07863851636648178, 0.0742821916937828, 0.07523003965616226, 0.08783216774463654, 0.08086008578538895, 0.16605475544929504]], [[0.10777217894792557, 0.1164829358458519, 0.10241525620222092, 0.1048697680234909, 0.12877261638641357, 0.10288788378238678, 0.09608875215053558, 0.13572509586811066, 0.016468998044729233, 0.0885164737701416], [0.10627574473619461, 0.11028560996055603, 0.09681695699691772, 0.09508325904607773, 0.14073830842971802, 0.11380008608102798, 0.10413898527622223, 0.1326913833618164, 0.01598973385989666, 0.08417993783950806], [0.10404772311449051, 0.10812189429998398, 0.09558924287557602, 0.09452077001333237, 0.13566812872886658, 0.11064371466636658, 0.10205669701099396, 0.14470800757408142, 0.020212125033140182, 0.08443168550729752], [0.1060829907655716, 0.11171060800552368, 0.10172685235738754, 0.10015898197889328, 0.13358788192272186, 0.1137285828590393, 0.10637165606021881, 0.13096174597740173, 0.023272953927516937, 0.07239770144224167], [0.09440205991268158, 0.09415507316589355, 0.07935306429862976, 0.07983926683664322, 0.12613163888454437, 0.0951763391494751, 0.086186982691288, 0.1758033037185669, 0.01772117055952549, 0.15123111009597778], [0.09082901477813721, 0.10592064261436462, 0.0950343981385231, 0.10147632658481598, 0.09894976019859314, 0.07544983178377151, 0.07574857026338577, 0.19657862186431885, 0.017011739313602448, 0.14300116896629333], [0.09111660718917847, 0.09459277987480164, 0.08165676146745682, 0.08444364368915558, 0.1124664694070816, 0.08626364916563034, 0.08076751977205276, 0.1983431875705719, 0.0244691651314497, 0.14588023722171783], [0.08582326024770737, 0.10099340230226517, 0.11163731664419174, 0.1302126795053482, 0.05722002312541008, 0.05987083911895752, 0.067438043653965, 0.1265861988067627, 0.1594693511724472, 0.10074880719184875], [0.08661941438913345, 0.09636399894952774, 0.10895460098981857, 0.11731579899787903, 0.060275848954916, 0.06652932614088058, 0.07576152682304382, 0.13266365230083466, 0.15092024207115173, 0.1045956090092659], [0.11494489759206772, 0.11919458955526352, 0.1282210499048233, 0.13655510544776917, 0.08697447180747986, 0.09641673415899277, 0.09984277188777924, 0.06288330256938934, 0.09081854671239853, 0.06414851546287537]]]], \"left_text\": [\"[CLS]\", \"my\", \"name\", \"is\", \"sara\", \".\", \"[SEP]\", \"0\", \"1\", \"2\"], \"right_text\": [\"[CLS]\", \"my\", \"name\", \"is\", \"sara\", \".\", \"[SEP]\", \"0\", \"1\", \"2\"]}], \"default_filter\": \"0\", \"display_mode\": \"dark\", \"root_div_id\": \"bertviz-9ba0410cc25c4dfda02c6c49542556b9\", \"include_layers\": [0], \"include_heads\": [0, 1, 2, 3, 4, 5, 6, 7], \"total_heads\": 8} is a template marker that is replaced by actual params.\n        const config = {};\n\n        const MIN_X = 0;\n        const MIN_Y = 0;\n        const DIV_WIDTH = 970;\n        const THUMBNAIL_PADDING = 5;\n        const DETAIL_WIDTH = 300;\n        const DETAIL_ATTENTION_WIDTH = 140;\n        const DETAIL_BOX_WIDTH = 80;\n        const DETAIL_BOX_HEIGHT = 18;\n        const DETAIL_PADDING = 15;\n        const ATTN_PADDING = 0;\n        const DETAIL_HEADING_HEIGHT = 25;\n        const HEADING_TEXT_SIZE = 15;\n        const HEADING_PADDING = 5;\n        const TEXT_SIZE = 13;\n        const TEXT_PADDING = 5;\n        const LAYER_COLORS = d3.schemeCategory10;\n        const PALETTE = {\n            'light': {\n                'text': 'black',\n                'background': 'white',\n                'highlight': '#F5F5F5'\n            },\n            'dark': {\n                'text': '#ccc',\n                'background': 'black',\n                'highlight': '#222'\n            }\n        }\n\n        function render() {\n\n            // Set global state variables\n\n            var attData = config.attention[config.filter];\n            config.leftText = attData.left_text;\n            config.rightText = attData.right_text;\n            config.attn = attData.attn;\n            config.numLayers = config.attn.length;\n            config.numHeads = config.attn[0].length;\n            config.thumbnailBoxHeight = 7 * (12 / config.totalHeads);\n            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;\n            config.thumbnailHeight = Math.max(config.leftText.length, config.rightText.length) * config.thumbnailBoxHeight + 2 * THUMBNAIL_PADDING;\n            config.thumbnailWidth = (DIV_WIDTH - axisSize) / config.totalHeads;\n            config.detailHeight = Math.max(config.leftText.length, config.rightText.length) * DETAIL_BOX_HEIGHT + 2 * DETAIL_PADDING + DETAIL_HEADING_HEIGHT;\n            config.divHeight = Math.max(config.numLayers * config.thumbnailHeight + axisSize, config.detailHeight);\n\n            const vis = $(`#${config.rootDivId} #vis`)\n            vis.empty();\n            vis.attr(\"height\", config.divHeight);\n            config.svg = d3.select(`#${config.rootDivId} #vis`)\n                .append('svg')\n                .attr(\"width\", DIV_WIDTH)\n                .attr(\"height\", config.divHeight)\n                .attr(\"fill\", getBackgroundColor());\n\n            renderAxisLabels();\n\n            var i;\n            var j;\n            for (i = 0; i < config.numLayers; i++) {\n                for (j = 0; j < config.numHeads; j++) {\n                    renderThumbnail(i, j);\n                }\n            }\n        }\n\n        function renderAxisLabels() {\n            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;\n            const tableWidth = config.thumbnailWidth * config.heads.length;\n            config.svg.append(\"text\")\n                .text(\"Heads\")\n                .attr(\"fill\", \"black\")\n                .attr(\"font-weight\", \"bold\")\n                .attr(\"font-size\", HEADING_TEXT_SIZE + \"px\")\n                .attr(\"x\", axisSize + tableWidth / 2)\n                .attr(\"text-anchor\", \"middle\")\n                .attr(\"y\", 0)\n                .attr(\"dy\", HEADING_TEXT_SIZE);\n            for (let i = 0; i < config.numHeads; i++) {\n                config.svg.append(\"text\")\n                    .text(config.heads[i])\n                    .attr(\"fill\", \"black\")\n                    .attr(\"font-size\", TEXT_SIZE + \"px\")\n                    .attr(\"x\", axisSize + (i + .5) * config.thumbnailWidth)\n                    .attr(\"text-anchor\", \"middle\")\n                    .attr(\"y\", HEADING_TEXT_SIZE + HEADING_PADDING)\n                    .attr(\"dy\", TEXT_SIZE);\n            }\n            let x = 0;\n            let y = axisSize + config.thumbnailHeight * config.layers.length / 2;\n            console.log(\"x\", x, y)\n            config.svg.append(\"text\")\n                .text(\"Layers\")\n                .attr(\"fill\", \"black\")\n                .attr(\"font-weight\", \"bold\")\n                .attr(\"transform\", \"rotate(270, \" + x  + \", \" + y + \")\")\n                .attr(\"font-size\", HEADING_TEXT_SIZE + \"px\")\n                .attr(\"x\", x)\n                .attr(\"text-anchor\", \"middle\")\n                .attr(\"y\", y)\n                .attr(\"dy\", HEADING_TEXT_SIZE);\n            for (let i = 0; i < config.numLayers; i++) {\n                x = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE; // HACK\n                y = axisSize + (i + .5) * config.thumbnailHeight;\n                config.svg.append(\"text\")\n                    .text(config.layers[i])\n                    .attr(\"fill\", \"black\")\n                    .attr(\"font-size\", TEXT_SIZE + \"px\")\n                    .attr(\"x\", x)\n                    .attr(\"text-anchor\", \"end\")\n                    .attr(\"y\", y)\n                    .attr(\"dy\", TEXT_SIZE / 2);\n            }\n        }\n\n\n        function renderThumbnail(layerIndex, headIndex) {\n            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING\n            const x = headIndex * config.thumbnailWidth + axisSize;\n            const y = layerIndex * config.thumbnailHeight + axisSize;\n            renderThumbnailAttn(x, y, config.attn[layerIndex][headIndex], layerIndex, headIndex);\n        }\n\n        function renderDetail(att, layerIndex, headIndex) {\n            const axisSize = TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;\n            var xOffset = .8 * config.thumbnailWidth;\n            var maxX = DIV_WIDTH;\n            var maxY = config.divHeight - 3;\n            var leftPos = axisSize + headIndex * config.thumbnailWidth;\n            var x = leftPos + THUMBNAIL_PADDING + xOffset;\n            if (x < MIN_X) {\n                x = MIN_X;\n            } else if (x + DETAIL_WIDTH > maxX) {\n                x = leftPos + THUMBNAIL_PADDING - DETAIL_WIDTH + 8;\n            }\n            var posLeftText = x;\n            var posAttention = posLeftText + DETAIL_BOX_WIDTH;\n            var posRightText = posAttention + DETAIL_ATTENTION_WIDTH;\n            var thumbnailHeight = Math.max(config.leftText.length, config.rightText.length) * config.thumbnailBoxHeight + 2 * THUMBNAIL_PADDING;\n            var yOffset = 20;\n            var y = layerIndex * thumbnailHeight + THUMBNAIL_PADDING + yOffset;\n            if (y < MIN_Y) {\n                y = MIN_Y;\n            } else if (y + config.detailHeight > maxY) {\n                y = maxY - config.detailHeight;\n            }\n            renderDetailFrame(x, y, layerIndex);\n            y = y + DETAIL_PADDING;\n            renderDetailHeading(x, y, layerIndex, headIndex);\n            y = y + DETAIL_HEADING_HEIGHT;\n            renderDetailText(config.leftText, \"leftText\", posLeftText, y , layerIndex);\n            renderDetailAttn(posAttention, y, att, layerIndex, headIndex);\n            renderDetailText(config.rightText, \"rightText\", posRightText, y, layerIndex);\n        }\n\n        function renderDetailHeading(x, y, layerIndex, headIndex) {\n            var fillColor = getTextColor();\n            config.svg.append(\"text\")\n                .classed(\"detail\", true)\n                .text('Layer ' + config.layers[layerIndex] + \", Head \" + config.heads[headIndex])\n                .attr(\"font-size\", TEXT_SIZE + \"px\")\n                .attr(\"font-weight\", \"bold\")\n                .style(\"cursor\", \"default\")\n                .style(\"-webkit-user-select\", \"none\")\n                .attr(\"fill\", fillColor)\n                .attr(\"x\", x + DETAIL_WIDTH / 2)\n                .attr(\"text-anchor\", \"middle\")\n                .attr(\"y\", y)\n                .attr(\"height\", DETAIL_HEADING_HEIGHT)\n                .attr(\"width\", DETAIL_WIDTH)\n                .attr(\"dy\", HEADING_TEXT_SIZE);\n        }\n\n        function renderDetailText(text, id, x, y, layerIndex) {\n            var tokenContainer = config.svg.append(\"svg:g\")\n                .classed(\"detail\", true)\n                .selectAll(\"g\")\n                .data(text)\n                .enter()\n                .append(\"g\");\n\n            var fillColor = getTextColor();\n\n            tokenContainer.append(\"rect\")\n                .classed(\"highlight\", true)\n                .attr(\"fill\", fillColor)\n                .style(\"opacity\", 0.0)\n                .attr(\"height\", DETAIL_BOX_HEIGHT)\n                .attr(\"width\", DETAIL_BOX_WIDTH)\n                .attr(\"x\", x)\n                .attr(\"y\", function (d, i) {\n                    return y + i * DETAIL_BOX_HEIGHT;\n                });\n\n            var textContainer = tokenContainer.append(\"text\")\n                .classed(\"token\", true)\n                .text(function (d) {\n                    return d;\n                })\n                .attr(\"font-size\", TEXT_SIZE + \"px\")\n                .style(\"cursor\", \"default\")\n                .style(\"-webkit-user-select\", \"none\")\n                .attr(\"fill\", fillColor)\n                .attr(\"x\", x)\n                .attr(\"y\", function (d, i) {\n                    return i * DETAIL_BOX_HEIGHT + y;\n                })\n                .attr(\"height\", DETAIL_BOX_HEIGHT)\n                .attr(\"width\", DETAIL_BOX_WIDTH)\n                .attr(\"dy\", TEXT_SIZE);\n\n            if (id == \"leftText\") {\n                textContainer.style(\"text-anchor\", \"end\")\n                    .attr(\"dx\", DETAIL_BOX_WIDTH - 2);\n                tokenContainer.on(\"mouseover\", function (d, index) {\n                    highlightSelection(index);\n                });\n                tokenContainer.on(\"mouseleave\", function () {\n                    unhighlightSelection();\n                });\n            }\n        }\n\n        function highlightSelection(index) {\n            config.svg.select(\"#leftText\")\n                .selectAll(\".highlight\")\n                .style(\"opacity\", function (d, i) {\n                    return i == index ? 1.0 : 0.0;\n                });\n            config.svg.selectAll(\".attn-line-group\")\n                .style(\"opacity\", function (d, i) {\n                    return i == index ? 1.0 : 0.0;\n                });\n        }\n\n        function unhighlightSelection() {\n            config.svg.select(\"#leftText\")\n                .selectAll(\".highlight\")\n                .style(\"opacity\", 0.0);\n            config.svg.selectAll(\".attn-line-group\")\n                .style(\"opacity\", 1);\n        }\n\n        function renderThumbnailAttn(x, y, att, layerIndex, headIndex) {\n\n            var attnContainer = config.svg.append(\"svg:g\");\n\n            var attnBackground = attnContainer.append(\"rect\")\n                .attr(\"id\", 'attn_background_' + layerIndex + \"_\" + headIndex)\n                .classed(\"attn_background\", true)\n                .attr(\"x\", x)\n                .attr(\"y\", y)\n                .attr(\"height\", config.thumbnailHeight)\n                .attr(\"width\", config.thumbnailWidth)\n                .attr(\"stroke-width\", 2)\n                .attr(\"stroke\", getLayerColor(layerIndex))\n                .attr(\"stroke-opacity\", 0)\n                .attr(\"fill\", getBackgroundColor());\n            var x1 = x + THUMBNAIL_PADDING;\n            var x2 = x1 + config.thumbnailWidth - 14;\n            var y1 = y + THUMBNAIL_PADDING;\n\n            attnContainer.selectAll(\"g\")\n                .data(att)\n                .enter()\n                .append(\"g\") // Add group for each source token\n                .attr(\"source-index\", function (d, i) { // Save index of source token\n                    return i;\n                })\n                .selectAll(\"line\")\n                .data(function (d) { // Loop over all target tokens\n                    return d;\n                })\n                .enter() // When entering\n                .append(\"line\")\n                .attr(\"x1\", x1)\n                .attr(\"y1\", function (d) {\n                    var sourceIndex = +this.parentNode.getAttribute(\"source-index\");\n                    return y1 + (sourceIndex + .5) * config.thumbnailBoxHeight;\n                })\n                .attr(\"x2\", x2)\n                .attr(\"y2\", function (d, targetIndex) {\n                    return y1 + (targetIndex + .5) * config.thumbnailBoxHeight;\n                })\n                .attr(\"stroke-width\", 2.2)\n                .attr(\"stroke\", getLayerColor(layerIndex))\n                .attr(\"stroke-opacity\", function (d) {\n                    return d;\n                });\n\n            var clickRegion = attnContainer.append(\"rect\")\n                .attr(\"x\", x)\n                .attr(\"y\", y)\n                .attr(\"height\", config.thumbnailHeight)\n                .attr(\"width\", config.thumbnailWidth)\n                .style(\"opacity\", 0);\n\n            clickRegion.on(\"click\", function (d, index) {\n                var attnBackgroundOther = config.svg.selectAll(\".attn_background\");\n                attnBackgroundOther.attr(\"fill\", getBackgroundColor());\n                attnBackgroundOther.attr(\"stroke-opacity\", 0);\n\n                config.svg.selectAll(\".detail\").remove();\n                if (config.detail_layer != layerIndex || config.detail_head != headIndex) {\n                    renderDetail(att, layerIndex, headIndex);\n                    config.detail_layer = layerIndex;\n                    config.detail_head = headIndex;\n                    attnBackground.attr(\"fill\", getHighlightColor());\n                    attnBackground.attr(\"stroke-opacity\", .8);\n                } else {\n                    config.detail_layer = null;\n                    config.detail_head = null;\n                    attnBackground.attr(\"fill\", getBackgroundColor());\n                    attnBackground.attr(\"stroke-opacity\", 0);\n                }\n            });\n\n            clickRegion.on(\"mouseover\", function (d) {\n                d3.select(this).style(\"cursor\", \"pointer\");\n            });\n        }\n\n        function renderDetailFrame(x, y, layerIndex) {\n            var detailFrame = config.svg.append(\"rect\")\n                .classed(\"detail\", true)\n                .attr(\"x\", x)\n                .attr(\"y\", y)\n                .attr(\"height\", config.detailHeight)\n                .attr(\"width\", DETAIL_WIDTH)\n                .style(\"opacity\", 1)\n                .attr(\"stroke-width\", 1.5)\n                .attr(\"stroke-opacity\", 0.7)\n                .attr(\"stroke\", getLayerColor(layerIndex));\n        }\n\n        function renderDetailAttn(x, y, att, layerIndex) {\n            var attnContainer = config.svg.append(\"svg:g\")\n                .classed(\"detail\", true)\n                .attr(\"pointer-events\", \"none\");\n            attnContainer.selectAll(\"g\")\n                .data(att)\n                .enter()\n                .append(\"g\") // Add group for each source token\n                .classed('attn-line-group', true)\n                .attr(\"source-index\", function (d, i) { // Save index of source token\n                    return i;\n                })\n                .selectAll(\"line\")\n                .data(function (d) { // Loop over all target tokens\n                    return d;\n                })\n                .enter()\n                .append(\"line\")\n                .attr(\"x1\", x + ATTN_PADDING)\n                .attr(\"y1\", function (d) {\n                    var sourceIndex = +this.parentNode.getAttribute(\"source-index\");\n                    return y + (sourceIndex + .5) * DETAIL_BOX_HEIGHT;\n                })\n                .attr(\"x2\", x + DETAIL_ATTENTION_WIDTH - ATTN_PADDING)\n                .attr(\"y2\", function (d, targetIndex) {\n                    return y + (targetIndex + .5) * DETAIL_BOX_HEIGHT;\n                })\n                .attr(\"stroke-width\", 2.2)\n                .attr(\"stroke\", getLayerColor(layerIndex))\n                .attr(\"stroke-opacity\", function (d) {\n                    return d;\n                });\n        }\n\n        function getLayerColor(layer) {\n          return LAYER_COLORS[config.layers[layer] % 10];\n        }\n\n        function getTextColor() {\n            return PALETTE[config.mode]['text']\n        }\n\n        function getBackgroundColor() {\n           return PALETTE[config.mode]['background']\n        }\n\n        function getHighlightColor() {\n           return PALETTE[config.mode]['highlight']\n        }\n\n        function initialize() {\n            config.attention = params['attention'];\n            config.filter = params['default_filter'];\n            config.mode = params['display_mode'];\n            config.layers = params['include_layers']\n            config.heads = params['include_heads']\n            config.totalHeads = params['total_heads']\n            config.rootDivId = params['root_div_id'];\n            $(`#${config.rootDivId} #filter`).on('change', function (e) {\n                config.filter = e.currentTarget.value;\n                render();\n            });\n        }\n\n        initialize();\n        render();\n\n    });",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_view([attention[1]], tokenizer.tokenize('[CLS] '+ text + ' [SEP]' + ''.join([' {}'.format(i) for i in range(sprectrome.shape[0])])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_view([attention[1]], tokenizer.tokenize('[CLS] '+ text + ' [SEP]'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kuhaku",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
