{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import sys\n",
    "\n",
    "lib_path = os.path.abspath(\"\").replace(\"notebooks\", \"src\")\n",
    "sys.path.append(lib_path)\n",
    "\n",
    "import torch\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score,confusion_matrix, f1_score, precision_score, recall_score\n",
    "from data.dataloader import build_train_test_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from models import networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def calculate_accuracy(y_pred, y_true):\n",
    "    class_weights = {cls: 1.0/count for cls, count in Counter(y_true).items()}\n",
    "    wa = balanced_accuracy_score(y_true, y_pred, sample_weight=[class_weights[cls] for cls in y_true])\n",
    "    ua = accuracy_score(y_true, y_pred)\n",
    "    return ua, wa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(cfg, checkpoint_path, all_state_dict=True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    network = getattr(networks, cfg.model_type)(cfg)\n",
    "    network.to(device)\n",
    "\n",
    "    # Build dataset\n",
    "    _, test_ds = build_train_test_dataset(cfg)\n",
    "    weight = torch.load(checkpoint_path, map_location=torch.device(device))\n",
    "    if all_state_dict:\n",
    "        weight = weight['state_dict_network']\n",
    "    else:\n",
    "        weight = weight.state_dict()\n",
    "    \n",
    "    network.load_state_dict(weight)\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    y_actu=[]\n",
    "    y_pred=[]\n",
    "\n",
    "    for every_test_list in tqdm(test_ds):\n",
    "        input_ids, audio, label = every_test_list\n",
    "        input_ids = input_ids.to(device)\n",
    "        audio = audio.to(device)\n",
    "        label = label.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = network(input_ids,audio)[0]\n",
    "            _, preds = torch.max(output, 1)\n",
    "            y_actu.append(label.detach().cpu().numpy()[0])\n",
    "            y_pred.append(preds.detach().cpu().numpy()[0])\n",
    "    bacc = balanced_accuracy_score(y_actu, y_pred)\n",
    "    print(\"Balanced Accuracy: \", bacc)\n",
    "    ua, wa = calculate_accuracy(y_actu, y_pred)\n",
    "    print(\"Unweighted Accuracy: \", ua)\n",
    "    print(\"Weighted Accuracy: \", wa)\n",
    "    ua_f1 = f1_score(y_actu, y_pred, average='macro')\n",
    "    # mean_f1 = np.mean(f1_score(y_actu, y_pred, average=None))\n",
    "    # w_f1 = f1_score(y_actu, y_pred, average='weighted')\n",
    "    # f1 = f1_score(y_actu, y_pred, average='micro')\n",
    "    # print(\"Micro F1: \", f1)\n",
    "    print(\"Macro F1: \", ua_f1)\n",
    "    # print(\"Weighted F1: \", w_f1)\n",
    "    # print(\"Mean F1:\", mean_f1)\n",
    "    # ua_precision = precision_score(y_actu, y_pred, average='macro')\n",
    "    # w_precision = precision_score(y_actu, y_pred, average='weighted')\n",
    "    # precision = precision_score(y_actu, y_pred, average='micro')\n",
    "    # mean_precision = np.mean(precision_score(y_actu, y_pred, average=None))\n",
    "    # print(\"Micro Precision: \", precision)\n",
    "    # print(\"Macro Precision: \", ua_precision)\n",
    "    # print(\"Weighted Precision: \", w_precision)\n",
    "    # print(\"Mean precision:\", mean_precision)\n",
    "    # ua_recall = recall_score(y_actu, y_pred, average='macro')\n",
    "    # w_recall = recall_score(y_actu, y_pred, average='weighted')\n",
    "    # recall = recall_score(y_actu, y_pred, average='micro')\n",
    "    # print(\"Micro Recall: \", recall)\n",
    "    # print(\"Macro Recall: \", ua_recall)\n",
    "    # print(\"Weighted Recall: \", w_recall)\n",
    "    \n",
    "    # cm = confusion_matrix(y_actu, y_pred)\n",
    "    # print(\"Confusion Matrix: \\n\", cm)\n",
    "    # cmn = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])*100\n",
    "\n",
    "    # ax = plt.subplots(figsize=(8, 5.5))[1]\n",
    "    # sns.heatmap(cmn, cmap='YlOrBr', annot=True, square=True, linecolor='black', linewidths=0.75, ax = ax, fmt = '.2f', annot_kws={'size': 16})\n",
    "    # ax.set_xlabel('Predicted', fontsize=18, fontweight='bold')\n",
    "    # ax.xaxis.set_label_position('bottom')\n",
    "    # ax.xaxis.set_ticklabels([\"Anger\", \"Happiness\", \"Sadness\", \"Neutral\"], fontsize=16)\n",
    "    # ax.set_ylabel('Ground Truth', fontsize=18, fontweight='bold')\n",
    "    # ax.yaxis.set_ticklabels([\"Anger\", \"Happiness\", \"Sadness\", \"Neutral\"], fontsize=16)\n",
    "    # plt.tight_layout()\n",
    "    # # plt.savefig(cfg.name + '.png', format='png', dpi=1200)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_svm(cfg, checkpoint_path, all_state_dict=True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    network = getattr(networks, cfg.model_type)(cfg)\n",
    "    network.to(device)\n",
    "\n",
    "    # Build dataset\n",
    "    train_ds, test_ds = build_train_test_dataset(cfg)\n",
    "    weight = torch.load(checkpoint_path, map_location=torch.device(device))\n",
    "    if all_state_dict:\n",
    "        weight = weight['state_dict_network']\n",
    "    else:\n",
    "        weight = weight.state_dict()\n",
    "    \n",
    "    network.load_state_dict(weight)\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    # Get train features\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    for every_train_list in tqdm(train_ds):\n",
    "        input_ids, audio, label = every_train_list\n",
    "        input_ids = input_ids.to(device)\n",
    "        audio = audio.to(device)\n",
    "        label = label.to(device)\n",
    "        with torch.no_grad():\n",
    "            feature = network(input_ids,audio)[1]\n",
    "            train_x.append(feature.detach().cpu().numpy()[0])\n",
    "            train_y.append(label.detach().cpu().numpy()[0])\n",
    "    \n",
    "    # SVM\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(train_x, train_y)\n",
    "    \n",
    "    y_actu=[]\n",
    "    y_pred=[]\n",
    "\n",
    "    for every_test_list in tqdm(test_ds):\n",
    "        input_ids, audio, label = every_test_list\n",
    "        input_ids = input_ids.to(device)\n",
    "        audio = audio.to(device)\n",
    "        label = label.to(device)\n",
    "        with torch.no_grad():\n",
    "            feature = network(input_ids,audio)[1]\n",
    "            preds = clf.predict(feature.detach().cpu().numpy())\n",
    "            y_actu.append(label.detach().cpu().numpy()[0])\n",
    "            y_pred.append(preds[0])\n",
    "    bacc = balanced_accuracy_score(y_actu, y_pred)\n",
    "    ua, wa = calculate_accuracy(y_actu, y_pred)\n",
    "    print(\"Balanced Accuracy: \", bacc)\n",
    "    print(\"Unweighted Accuracy: \", ua)\n",
    "    print(\"Weighted Accuracy: \", wa)\n",
    "    \n",
    "    ua_f1 = f1_score(y_actu, y_pred, average='macro')\n",
    "    # w_f1 = f1_score(y_actu, y_pred, average='weighted')\n",
    "    # f1 = f1_score(y_actu, y_pred, average='micro')\n",
    "    # print(\"Micro F1: \", f1)\n",
    "    print(\"Macro F1: \", ua_f1)\n",
    "    # print(\"Weighted F1: \", w_f1)\n",
    "    # ua_precision = precision_score(y_actu, y_pred, average='macro')\n",
    "    # w_precision = precision_score(y_actu, y_pred, average='weighted')\n",
    "    # precision = precision_score(y_actu, y_pred, average='micro')\n",
    "    # print(\"Micro Precision: \", precision)\n",
    "    # print(\"Macro Precision: \", ua_precision)\n",
    "    # print(\"Weighted Precision: \", w_precision)\n",
    "    # ua_recall = recall_score(y_actu, y_pred, average='macro')\n",
    "    # w_recall = recall_score(y_actu, y_pred, average='weighted')\n",
    "    # recall = recall_score(y_actu, y_pred, average='micro')\n",
    "    # print(\"Micro Recall: \", recall)\n",
    "    # print(\"Macro Recall: \", ua_recall)\n",
    "    # print(\"Weighted Recall: \", w_recall)\n",
    "    \n",
    "    # cm = confusion_matrix(y_actu, y_pred)\n",
    "    # print(\"Confusion Matrix: \\n\", cm)\n",
    "    # cmn = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])*100\n",
    "\n",
    "    # ax = plt.subplots(figsize=(8, 5.5))[1]\n",
    "    # sns.heatmap(cmn, cmap='YlOrBr', annot=True, square=True, linecolor='black', linewidths=0.75, ax = ax, fmt = '.2f', annot_kws={'size': 16})\n",
    "    # ax.set_xlabel('Predicted', fontsize=18, fontweight='bold')\n",
    "    # ax.xaxis.set_label_position('bottom')\n",
    "    # ax.xaxis.set_ticklabels([\"Anger\", \"Happiness\", \"Sadness\", \"Neutral\"], fontsize=16)\n",
    "    # ax.set_ylabel('Ground Truth', fontsize=18, fontweight='bold')\n",
    "    # ax.yaxis.set_ticklabels([\"Anger\", \"Happiness\", \"Sadness\", \"Neutral\"], fontsize=16)\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(cfg.name + '.png', format='png', dpi=1200)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 554/554 [00:12<00:00, 44.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy:  0.7961631280620685\n",
      "Unweighted Accuracy:  0.7924187725631769\n",
      "Weighted Accuracy:  0.8009065107524492\n",
      "Macro F1:  0.7963316032664567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from configs.base import Config\n",
    "checkpoint_path = \"/home/namphuongtran9196/Code/EmotionClassification/code/3m-ser-private/scripts/checkpoints/3M-SER/CrossEntropyLoss_bert_hubert_base_cls/20240127-112454\"\n",
    "cfg_path = os.path.join(checkpoint_path,\"cfg.log\")\n",
    "ckpt_path = os.path.join(checkpoint_path,\"weights/best_acc/checkpoint_0_0.pt\")\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "cfg.load(cfg_path)\n",
    "# Set dataset path\n",
    "cfg.data_root=\"/home/namphuongtran9196/Code/EmotionClassification/code/3m-ser-private/scripts/data/IEMOCAP_preprocessed\"\n",
    "# Change to test set\n",
    "cfg.data_valid=\"test.pkl\"\n",
    "\n",
    "eval(cfg, ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 554/554 [00:08<00:00, 67.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy:  0.7702337657537457\n",
      "Unweighted Accuracy:  0.7671480144404332\n",
      "Weighted Accuracy:  0.7787450643734654\n",
      "Macro F1:  0.7701193146838053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from configs.base import Config\n",
    "checkpoint_path = \"/home/namphuongtran9196/Code/EmotionClassification/code/3m-ser-private/scripts/checkpoints/3M-SER/CrossEntropyLoss_bert_vggish_cls/20240127-112233\"\n",
    "cfg_path = os.path.join(checkpoint_path,\"cfg.log\")\n",
    "ckpt_path = os.path.join(checkpoint_path,\"weights/best_acc/checkpoint_0_0.pt\")\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "cfg.load(cfg_path)\n",
    "# Set dataset path\n",
    "cfg.data_root=\"/home/namphuongtran9196/Code/EmotionClassification/code/3m-ser-private/scripts/data/IEMOCAP_preprocessed\"\n",
    "# Change to test set\n",
    "cfg.data_valid=\"test.pkl\"\n",
    "\n",
    "eval(cfg, ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 554/554 [00:12<00:00, 44.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy:  0.7476584744596645\n",
      "Unweighted Accuracy:  0.7364620938628159\n",
      "Weighted Accuracy:  0.7503535596999569\n",
      "Macro F1:  0.7435537095207938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from configs.base import Config\n",
    "checkpoint_path = \"/home/namphuongtran9196/Code/EmotionClassification/code/3m-ser-private/scripts/checkpoints/3M-SER/CrossEntropyLoss_bert_wav2vec2_base_cls/20240127-112408\"\n",
    "opt_path = os.path.join(checkpoint_path,\"cfg.log\")\n",
    "ckpt_path = os.path.join(checkpoint_path,\"weights/best_acc/checkpoint_0_0.pt\")\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "cfg.load(opt_path)\n",
    "# Set dataset path\n",
    "cfg.data_root=\"/home/namphuongtran9196/Code/EmotionClassification/code/3m-ser-private/scripts/data/IEMOCAP_preprocessed\"\n",
    "# Change to test set\n",
    "cfg.data_valid=\"test.pkl\"\n",
    "\n",
    "eval(cfg, ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 554/554 [00:13<00:00, 41.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy:  0.7630128329663103\n",
      "Unweighted Accuracy:  0.7545126353790613\n",
      "Weighted Accuracy:  0.7696427097256077\n",
      "Macro F1:  0.764027027268112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from configs.base import Config\n",
    "checkpoint_path = \"/home/namphuongtran9196/Code/EmotionClassification/code/3m-ser-private/scripts/checkpoints/3M-SER/CrossEntropyLoss_bert_wavlm_base_cls/20240127-112336\"\n",
    "opt_path = os.path.join(checkpoint_path,\"cfg.log\")\n",
    "ckpt_path = os.path.join(checkpoint_path,\"weights/best_acc/checkpoint_0_0.pt\")\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "cfg.load(opt_path)\n",
    "# Set dataset path\n",
    "cfg.data_root=\"/home/namphuongtran9196/Code/EmotionClassification/code/3m-ser-private/scripts/data/IEMOCAP_preprocessed\"\n",
    "# Change to test set\n",
    "cfg.data_valid=\"test.pkl\"\n",
    "\n",
    "eval(cfg, ckpt_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
