{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/namphuongtran9196/.miniconda3/envs/SER/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import sys\n",
    "\n",
    "lib_path = os.path.abspath(\"\").replace(\"notebooks\", \"src\")\n",
    "sys.path.append(lib_path)\n",
    "\n",
    "import torch\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score,confusion_matrix, f1_score, precision_score, recall_score\n",
    "from data.dataloader import build_train_test_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from models import networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def calculate_accuracy(y_pred, y_true):\n",
    "    class_weights = {cls: 1.0/count for cls, count in Counter(y_true).items()}\n",
    "    wa = balanced_accuracy_score(y_true, y_pred, sample_weight=[class_weights[cls] for cls in y_true])\n",
    "    ua = accuracy_score(y_true, y_pred)\n",
    "    return ua, wa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(cfg, checkpoint_path, all_state_dict=True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    network = getattr(networks, cfg.model_type)(cfg)\n",
    "    network.to(device)\n",
    "\n",
    "    # Build dataset\n",
    "    _, test_ds = build_train_test_dataset(cfg)\n",
    "    weight = torch.load(checkpoint_path, map_location=torch.device(device))\n",
    "    if all_state_dict:\n",
    "        weight = weight['state_dict_network']\n",
    "    else:\n",
    "        weight = weight.state_dict()\n",
    "    \n",
    "    network.load_state_dict(weight)\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    y_actu=[]\n",
    "    y_pred=[]\n",
    "\n",
    "    for every_test_list in tqdm(test_ds):\n",
    "        input_ids, audio, label = every_test_list\n",
    "        input_ids = input_ids.to(device)\n",
    "        audio = audio.to(device)\n",
    "        label = label.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = network(input_ids,audio)[0]\n",
    "            _, preds = torch.max(output, 1)\n",
    "            y_actu.append(label.detach().cpu().numpy()[0])\n",
    "            y_pred.append(preds.detach().cpu().numpy()[0])\n",
    "    bacc = balanced_accuracy_score(y_actu, y_pred)\n",
    "    print(\"Balanced Accuracy: \", bacc)\n",
    "    ua, wa = calculate_accuracy(y_actu, y_pred)\n",
    "    print(\"Unweighted Accuracy: \", ua)\n",
    "    print(\"Weighted Accuracy: \", wa)\n",
    "    ua_f1 = f1_score(y_actu, y_pred, average='macro')\n",
    "    # mean_f1 = np.mean(f1_score(y_actu, y_pred, average=None))\n",
    "    # w_f1 = f1_score(y_actu, y_pred, average='weighted')\n",
    "    # f1 = f1_score(y_actu, y_pred, average='micro')\n",
    "    # print(\"Micro F1: \", f1)\n",
    "    print(\"Macro F1: \", ua_f1)\n",
    "    # print(\"Weighted F1: \", w_f1)\n",
    "    # print(\"Mean F1:\", mean_f1)\n",
    "    # ua_precision = precision_score(y_actu, y_pred, average='macro')\n",
    "    # w_precision = precision_score(y_actu, y_pred, average='weighted')\n",
    "    # precision = precision_score(y_actu, y_pred, average='micro')\n",
    "    # mean_precision = np.mean(precision_score(y_actu, y_pred, average=None))\n",
    "    # print(\"Micro Precision: \", precision)\n",
    "    # print(\"Macro Precision: \", ua_precision)\n",
    "    # print(\"Weighted Precision: \", w_precision)\n",
    "    # print(\"Mean precision:\", mean_precision)\n",
    "    # ua_recall = recall_score(y_actu, y_pred, average='macro')\n",
    "    # w_recall = recall_score(y_actu, y_pred, average='weighted')\n",
    "    # recall = recall_score(y_actu, y_pred, average='micro')\n",
    "    # print(\"Micro Recall: \", recall)\n",
    "    # print(\"Macro Recall: \", ua_recall)\n",
    "    # print(\"Weighted Recall: \", w_recall)\n",
    "    \n",
    "    # cm = confusion_matrix(y_actu, y_pred)\n",
    "    # print(\"Confusion Matrix: \\n\", cm)\n",
    "    # cmn = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])*100\n",
    "\n",
    "    # ax = plt.subplots(figsize=(8, 5.5))[1]\n",
    "    # sns.heatmap(cmn, cmap='YlOrBr', annot=True, square=True, linecolor='black', linewidths=0.75, ax = ax, fmt = '.2f', annot_kws={'size': 16})\n",
    "    # ax.set_xlabel('Predicted', fontsize=18, fontweight='bold')\n",
    "    # ax.xaxis.set_label_position('bottom')\n",
    "    # ax.xaxis.set_ticklabels([\"Anger\", \"Happiness\", \"Sadness\", \"Neutral\"], fontsize=16)\n",
    "    # ax.set_ylabel('Ground Truth', fontsize=18, fontweight='bold')\n",
    "    # ax.yaxis.set_ticklabels([\"Anger\", \"Happiness\", \"Sadness\", \"Neutral\"], fontsize=16)\n",
    "    # plt.tight_layout()\n",
    "    # # plt.savefig(cfg.name + '.png', format='png', dpi=1200)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_svm(cfg, checkpoint_path, all_state_dict=True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    network = getattr(networks, cfg.model_type)(cfg)\n",
    "    network.to(device)\n",
    "\n",
    "    # Build dataset\n",
    "    train_ds, test_ds = build_train_test_dataset(cfg)\n",
    "    weight = torch.load(checkpoint_path, map_location=torch.device(device))\n",
    "    if all_state_dict:\n",
    "        weight = weight['state_dict_network']\n",
    "    else:\n",
    "        weight = weight.state_dict()\n",
    "    \n",
    "    network.load_state_dict(weight)\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "    \n",
    "    network2 = getattr(networks, cfg.model_type)(cfg)\n",
    "    network2.to(device)\n",
    "\n",
    "    # Get train features\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    for every_train_list in tqdm(train_ds):\n",
    "        input_ids, audio, label = every_train_list\n",
    "        input_ids = input_ids.to(device)\n",
    "        audio = audio.to(device)\n",
    "        label = label.to(device)\n",
    "        with torch.no_grad():\n",
    "            feature = network(input_ids,audio)[1]\n",
    "            train_x.append(feature.detach().cpu().numpy()[0])\n",
    "            train_y.append(label.detach().cpu().numpy()[0])\n",
    "    \n",
    "    # SVM\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(train_x, train_y)\n",
    "    \n",
    "    y_actu=[]\n",
    "    y_pred=[]\n",
    "\n",
    "    for every_test_list in tqdm(test_ds):\n",
    "        input_ids, audio, label = every_test_list\n",
    "        input_ids = input_ids.to(device)\n",
    "        audio = audio.to(device)\n",
    "        label = label.to(device)\n",
    "        with torch.no_grad():\n",
    "            feature = network(input_ids,audio)[1]\n",
    "            preds = clf.predict(feature.detach().cpu().numpy())\n",
    "            y_actu.append(label.detach().cpu().numpy()[0])\n",
    "            y_pred.append(preds[0])\n",
    "    bacc = balanced_accuracy_score(y_actu, y_pred)\n",
    "    ua, wa = calculate_accuracy(y_actu, y_pred)\n",
    "    print(\"Balanced Accuracy: \", bacc)\n",
    "    print(\"Unweighted Accuracy: \", ua)\n",
    "    print(\"Weighted Accuracy: \", wa)\n",
    "    \n",
    "    ua_f1 = f1_score(y_actu, y_pred, average='macro')\n",
    "    # w_f1 = f1_score(y_actu, y_pred, average='weighted')\n",
    "    # f1 = f1_score(y_actu, y_pred, average='micro')\n",
    "    # print(\"Micro F1: \", f1)\n",
    "    print(\"Macro F1: \", ua_f1)\n",
    "    # print(\"Weighted F1: \", w_f1)\n",
    "    # ua_precision = precision_score(y_actu, y_pred, average='macro')\n",
    "    # w_precision = precision_score(y_actu, y_pred, average='weighted')\n",
    "    # precision = precision_score(y_actu, y_pred, average='micro')\n",
    "    # print(\"Micro Precision: \", precision)\n",
    "    # print(\"Macro Precision: \", ua_precision)\n",
    "    # print(\"Weighted Precision: \", w_precision)\n",
    "    # ua_recall = recall_score(y_actu, y_pred, average='macro')\n",
    "    # w_recall = recall_score(y_actu, y_pred, average='weighted')\n",
    "    # recall = recall_score(y_actu, y_pred, average='micro')\n",
    "    # print(\"Micro Recall: \", recall)\n",
    "    # print(\"Macro Recall: \", ua_recall)\n",
    "    # print(\"Weighted Recall: \", w_recall)\n",
    "    \n",
    "    # cm = confusion_matrix(y_actu, y_pred)\n",
    "    # print(\"Confusion Matrix: \\n\", cm)\n",
    "    # cmn = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])*100\n",
    "\n",
    "    # ax = plt.subplots(figsize=(8, 5.5))[1]\n",
    "    # sns.heatmap(cmn, cmap='YlOrBr', annot=True, square=True, linecolor='black', linewidths=0.75, ax = ax, fmt = '.2f', annot_kws={'size': 16})\n",
    "    # ax.set_xlabel('Predicted', fontsize=18, fontweight='bold')\n",
    "    # ax.xaxis.set_label_position('bottom')\n",
    "    # ax.xaxis.set_ticklabels([\"Anger\", \"Happiness\", \"Sadness\", \"Neutral\"], fontsize=16)\n",
    "    # ax.set_ylabel('Ground Truth', fontsize=18, fontweight='bold')\n",
    "    # ax.yaxis.set_ticklabels([\"Anger\", \"Happiness\", \"Sadness\", \"Neutral\"], fontsize=16)\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(cfg.name + '.png', format='png', dpi=1200)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p1, p2 = zip(model1.parameters(), model2.parameters()):\n",
    "    if p1.data.ne(p2.data).sum() > 0:\n",
    "        return False\n",
    "return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/namphuongtran9196/Code/EmotionClassification/code/3m-ser-private/scripts/checkpoints/3M-SER/CrossEntropyLoss_bert_hubert_base_cls/20240127-112454/cfg.log'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_path,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights/best_acc/checkpoint_0_0.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m cfg \u001b[38;5;241m=\u001b[39m Config()\n\u001b[0;32m----> 8\u001b[0m \u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Set dataset path\u001b[39;00m\n\u001b[1;32m     10\u001b[0m cfg\u001b[38;5;241m.\u001b[39mdata_root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/namphuongtran9196/Code/EmotionClassification/code/3m-ser-private/scripts/data/IEMOCAP_preprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Code/EmotionClassification/code/3m-ser-private/src/configs/base.py:64\u001b[0m, in \u001b[0;36mBaseConfig.load\u001b[0;34m(self, cfg_path)\u001b[0m\n\u001b[1;32m     61\u001b[0m         value \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     65\u001b[0m     data \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# remove all empty strings\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/namphuongtran9196/Code/EmotionClassification/code/3m-ser-private/scripts/checkpoints/3M-SER/CrossEntropyLoss_bert_hubert_base_cls/20240127-112454/cfg.log'"
     ]
    }
   ],
   "source": [
    "from configs.base import Config\n",
    "checkpoint_path = \"/home/namphuongtran9196/Code/EmotionClassification/code/3m-ser-private/scripts/checkpoints_latest/IEMOCAP/3M-SER/CrossEntropyLoss_bert_vggish_cls/20240129-160154\"\n",
    "cfg_path = os.path.join(checkpoint_path,\"cfg.log\")\n",
    "ckpt_path = os.path.join(checkpoint_path,\"weights/best_acc/checkpoint_0_0.pt\")\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "cfg.load(cfg_path)\n",
    "# Set dataset path\n",
    "cfg.data_root=\"/home/namphuongtran9196/Code/EmotionClassification/code/3m-ser-private/scripts/data/IEMOCAP_preprocessed\"\n",
    "# Change to test set\n",
    "cfg.data_valid=\"test.pkl\"\n",
    "\n",
    "eval(cfg, ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 554/554 [00:08<00:00, 67.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy:  0.7702337657537457\n",
      "Unweighted Accuracy:  0.7671480144404332\n",
      "Weighted Accuracy:  0.7787450643734654\n",
      "Macro F1:  0.7701193146838053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from configs.base import Config\n",
    "checkpoint_path = \"/home/namphuongtran9196/Code/EmotionClassification/code/3m-ser-private/scripts/checkpoints/3M-SER/CrossEntropyLoss_bert_vggish_cls/20240127-112233\"\n",
    "cfg_path = os.path.join(checkpoint_path,\"cfg.log\")\n",
    "ckpt_path = os.path.join(checkpoint_path,\"weights/best_acc/checkpoint_0_0.pt\")\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "cfg.load(cfg_path)\n",
    "# Set dataset path\n",
    "cfg.data_root=\"/home/namphuongtran9196/Code/EmotionClassification/code/3m-ser-private/scripts/data/IEMOCAP_preprocessed\"\n",
    "# Change to test set\n",
    "cfg.data_valid=\"test.pkl\"\n",
    "\n",
    "eval(cfg, ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 554/554 [00:12<00:00, 44.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy:  0.7476584744596645\n",
      "Unweighted Accuracy:  0.7364620938628159\n",
      "Weighted Accuracy:  0.7503535596999569\n",
      "Macro F1:  0.7435537095207938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from configs.base import Config\n",
    "checkpoint_path = \"/home/namphuongtran9196/Code/EmotionClassification/code/3m-ser-private/scripts/checkpoints/3M-SER/CrossEntropyLoss_bert_wav2vec2_base_cls/20240127-112408\"\n",
    "opt_path = os.path.join(checkpoint_path,\"cfg.log\")\n",
    "ckpt_path = os.path.join(checkpoint_path,\"weights/best_acc/checkpoint_0_0.pt\")\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "cfg.load(opt_path)\n",
    "# Set dataset path\n",
    "cfg.data_root=\"/home/namphuongtran9196/Code/EmotionClassification/code/3m-ser-private/scripts/data/IEMOCAP_preprocessed\"\n",
    "# Change to test set\n",
    "cfg.data_valid=\"test.pkl\"\n",
    "\n",
    "eval(cfg, ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 554/554 [00:13<00:00, 41.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy:  0.7630128329663103\n",
      "Unweighted Accuracy:  0.7545126353790613\n",
      "Weighted Accuracy:  0.7696427097256077\n",
      "Macro F1:  0.764027027268112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from configs.base import Config\n",
    "checkpoint_path = \"/home/namphuongtran9196/Code/EmotionClassification/code/3m-ser-private/scripts/checkpoints/3M-SER/CrossEntropyLoss_bert_wavlm_base_cls/20240127-112336\"\n",
    "opt_path = os.path.join(checkpoint_path,\"cfg.log\")\n",
    "ckpt_path = os.path.join(checkpoint_path,\"weights/best_acc/checkpoint_0_0.pt\")\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "cfg.load(opt_path)\n",
    "# Set dataset path\n",
    "cfg.data_root=\"/home/namphuongtran9196/Code/EmotionClassification/code/3m-ser-private/scripts/data/IEMOCAP_preprocessed\"\n",
    "# Change to test set\n",
    "cfg.data_valid=\"test.pkl\"\n",
    "\n",
    "eval(cfg, ckpt_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
